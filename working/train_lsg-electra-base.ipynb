{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b8b30af-743a-45b8-9710-fc3567665c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import gc\n",
    "# import re\n",
    "# import ast\n",
    "# import sys\n",
    "# import copy\n",
    "# import json\n",
    "# import time\n",
    "# import math\n",
    "# import string\n",
    "# import pickle\n",
    "# import random\n",
    "# import joblib\n",
    "# import itertools\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# import scipy as sp\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from tqdm.auto import tqdm\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.nn import Parameter\n",
    "# import torch.nn.functional as F\n",
    "# from torch.optim import Adam, SGD, AdamW\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "# import tokenizers\n",
    "# import transformers\n",
    "# from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "# from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "# from transformers import DataCollatorWithPadding\n",
    "# os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# import codecs\n",
    "# from typing import Dict, List, Tuple\n",
    "# from text_unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82c22ea-988d-44ef-acb5-2b996ea545de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.21.2\n",
    "# !pip install tokenizers==0.12.1\n",
    "\n",
    "# !pip install -q joblib scikit-learn scipy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09ac4f52-194d-41e4-82d1-fda0adb599c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://repo.huaweicloud.com/repository/pypi/simple\n",
      "Requirement already satisfied: iterative-stratification==0.1.7 in /root/miniconda3/lib/python3.8/site-packages (0.1.7)\n",
      "Requirement already satisfied: scipy in /root/miniconda3/lib/python3.8/site-packages (from iterative-stratification==0.1.7) (1.9.3)\n",
      "Requirement already satisfied: numpy in /root/miniconda3/lib/python3.8/site-packages (from iterative-stratification==0.1.7) (1.22.4)\n",
      "Requirement already satisfied: scikit-learn in /root/miniconda3/lib/python3.8/site-packages (from iterative-stratification==0.1.7) (1.1.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /root/miniconda3/lib/python3.8/site-packages (from scikit-learn->iterative-stratification==0.1.7) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /root/miniconda3/lib/python3.8/site-packages (from scikit-learn->iterative-stratification==0.1.7) (1.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers.__version__: 0.12.1\n",
      "transformers.__version__: 4.21.2\n",
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "os.system('pip install iterative-stratification==0.1.7')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "import tokenizers\n",
    "import transformers\n",
    "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc1574b-51ec-4fd1-aed7-c56f30354740",
   "metadata": {},
   "source": [
    "# CFG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bec36cda-23f1-4e77-b26c-c8e46b01844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_P = '/root/autodl-tmp/fb3/inputs/'\n",
    "OUTPUT_DIR = '/root/autodl-tmp/fb3/output/trained_tiny_model/lsg-electra-base/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5f643ff-884a-4d3d-a464-30cff8ca30af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    wandb=False\n",
    "    # competition='FB3'\n",
    "    # _wandb_kernel='nakama'\n",
    "    \n",
    "    path=f\"{DATA_P}common-nlp-tokenizer/model_tokenizer/tiny/lsg-electra-base/\"\n",
    "    model=path\n",
    "    config_path=model+'config.pth'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    cfg_save_output = OUTPUT_DIR\n",
    "\n",
    "    debug=False\n",
    "    train=True\n",
    "    trust_remote_code=True\n",
    "    \n",
    "    apex=True\n",
    "    print_freq=20\n",
    "    num_workers=4\n",
    "    gradient_checkpointing=True\n",
    "    scheduler='cosine' # ['linear', 'cosine']\n",
    "    batch_scheduler=True\n",
    "    num_cycles=0.5\n",
    "    num_warmup_steps=0\n",
    "    epochs=4\n",
    "    encoder_lr=2e-5\n",
    "    decoder_lr=2e-5\n",
    "    min_lr=1e-6\n",
    "    eps=1e-6\n",
    "    betas=(0.9, 0.999)\n",
    "    batch_size=8\n",
    "    weight_decay=0.01\n",
    "    gradient_accumulation_steps=1\n",
    "    max_grad_norm=1000\n",
    "    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "    seed=42\n",
    "    n_fold=4\n",
    "    trn_fold=[0, 1, 2, 3]\n",
    "    max_len=1436\n",
    "    \n",
    "if CFG.debug:\n",
    "    CFG.epochs = 2\n",
    "    CFG.trn_fold = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd54f4c9-0e59-4750-8fd3-fa19763cf32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ====================================================\n",
    "# # tokenizer\n",
    "# # ====================================================\n",
    "# tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
    "# tokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\n",
    "# CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1079d60-08d8-44e9-be5a-d59a1a1e388d",
   "metadata": {},
   "source": [
    "# Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5883b61b-c09f-416d-81f0-f2292d0fc887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "\n",
    "def MCRMSE(y_trues, y_preds):\n",
    "    scores = []\n",
    "    idxes = y_trues.shape[1]\n",
    "    for i in range(idxes):\n",
    "        y_true = y_trues[:,i]\n",
    "        y_pred = y_preds[:,i]\n",
    "        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n",
    "        scores.append(score)\n",
    "    mcrmse_score = np.mean(scores)\n",
    "    return mcrmse_score, scores\n",
    "\n",
    "\n",
    "def get_score(y_trues, y_preds):\n",
    "    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n",
    "    return mcrmse_score, scores\n",
    "\n",
    "\n",
    "def get_logger(filename=OUTPUT_DIR+'train'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbd48c0-7b8c-4361-8a0a-d3b1930a267c",
   "metadata": {},
   "source": [
    "##  Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b5c4856-a18e-4552-9dff-1f31e9a4ecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Data Loading\n",
    "# ====================================================\n",
    "train = pd.read_csv(f'{DATA_P}feedback-prize-english-language-learning/train.csv')\n",
    "test = pd.read_csv(f'{DATA_P}feedback-prize-english-language-learning/test.csv')\n",
    "submission = pd.read_csv(f'{DATA_P}feedback-prize-english-language-learning/sample_submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4318c6c2-679b-4689-ae5e-459463304d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    978\n",
       "1    977\n",
       "2    978\n",
       "3    978\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# CV split\n",
    "# ====================================================\n",
    "Fold = MultilabelStratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train[CFG.target_cols])):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "display(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "765402fa-a541-45ee-88ca-305bee3113a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    display(train.groupby('fold').size())\n",
    "    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n",
    "    display(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f719884f-ff13-4742-b9bf-e346b29bbe23",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e231bc7-d317-4aa1-b0e6-ffd670974143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2456389dfa2448a92cf0e61a923461a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3911 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_len: 1436\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Define max_len\n",
    "# ====================================================\n",
    "# lengths = []\n",
    "# tk0 = tqdm(train['full_text'].fillna(\"\").values, total=len(train))\n",
    "# for text in tk0:\n",
    "#     length = len(CFG.tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "#     lengths.append(length)\n",
    "# CFG.max_len = max(lengths) + 3 # cls & sep & sep\n",
    "LOGGER.info(f\"max_len: {CFG.max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dff7a599-7bf6-4e9e-bc08-c6eb2742d298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "def prepare_input(cfg, text):\n",
    "    inputs = cfg.tokenizer.encode_plus(\n",
    "        text, \n",
    "        return_tensors=None, \n",
    "        add_special_tokens=True, \n",
    "        max_length=CFG.max_len,\n",
    "        pad_to_max_length=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.texts = df['full_text'].values\n",
    "        self.labels = df[cfg.target_cols].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.cfg, self.texts[item])\n",
    "        label = torch.tensor(self.labels[item], dtype=torch.float)\n",
    "        return inputs, label\n",
    "    \n",
    "\n",
    "def collate(inputs):\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b00ae5-9cb9-49f7-8831-4f5b8991965c",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d72c2e5e-aeca-4fa8-bd47-051b40d7b328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "    \n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True, trust_remote_code=True)\n",
    "            self.config.hidden_dropout = 0.\n",
    "            self.config.hidden_dropout_prob = 0.\n",
    "            self.config.attention_dropout = 0.\n",
    "            self.config.attention_probs_dropout_prob = 0.\n",
    "            LOGGER.info(self.config)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(cfg.model, config=self.config, trust_remote_code=True)\n",
    "        else:\n",
    "            self.model = AutoModel(self.config)\n",
    "        if self.cfg.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        self.pool = MeanPooling()\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 6)\n",
    "        self._init_weights(self.fc)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        feature = self.pool(last_hidden_states, inputs['attention_mask'])\n",
    "        return feature\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(feature)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "# ====================================================\n",
    "# Loss\n",
    "# ====================================================\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, reduction='mean', eps=1e-9):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss(reduction='none')\n",
    "        self.reduction = reduction\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = torch.sqrt(self.mse(y_pred, y_true) + self.eps)\n",
    "        if self.reduction == 'none':\n",
    "            loss = loss\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "# ====================================================\n",
    "# Helper functions\n",
    "# ====================================================\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            y_preds = model(inputs)\n",
    "            loss = criterion(y_preds, labels)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            if CFG.batch_scheduler:\n",
    "                scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch+1, step, len(train_loader), \n",
    "                          remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "        if CFG.wandb:\n",
    "            wandb.log({f\"[fold{fold}] loss\": losses.val,\n",
    "                       f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs, labels) in enumerate(valid_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "            loss = criterion(y_preds, labels)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbf1214-951b-472a-b2e6-33581fbb4be3",
   "metadata": {},
   "source": [
    "# train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dfe6437-8227-4eef-9789-182c9ae029f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# train loop\n",
    "# ====================================================\n",
    "def train_loop(folds, fold):\n",
    "    \n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n",
    "    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n",
    "    valid_labels = valid_folds[CFG.target_cols].values\n",
    "    \n",
    "    train_dataset = TrainDataset(CFG, train_folds)\n",
    "    valid_dataset = TrainDataset(CFG, valid_folds)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    model = CustomModel(CFG, config_path=None, pretrained=True)\n",
    "    torch.save(model.config, OUTPUT_DIR+'config.pth')\n",
    "    model.to(device)\n",
    "    \n",
    "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "             'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "\n",
    "    optimizer_parameters = get_optimizer_params(model,\n",
    "                                                encoder_lr=CFG.encoder_lr, \n",
    "                                                decoder_lr=CFG.decoder_lr,\n",
    "                                                weight_decay=CFG.weight_decay)\n",
    "    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n",
    "    \n",
    "    # ====================================================\n",
    "    # scheduler\n",
    "    # ====================================================\n",
    "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        if cfg.scheduler == 'linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "            )\n",
    "        elif cfg.scheduler == 'cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "            )\n",
    "        return scheduler\n",
    "    \n",
    "    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    criterion = nn.SmoothL1Loss(reduction='mean') # RMSELoss(reduction=\"mean\")\n",
    "    \n",
    "    best_score = np.inf\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n",
    "        \n",
    "        # scoring\n",
    "        score, scores = get_score(valid_labels, predictions)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}  Scores: {scores}')\n",
    "        if CFG.wandb:\n",
    "            wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n",
    "                       f\"[fold{fold}] avg_train_loss\": avg_loss, \n",
    "                       f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n",
    "                       f\"[fold{fold}] score\": score})\n",
    "        \n",
    "        if best_score > score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                        f\"{CFG.cfg_save_output}_fold{fold}_best.pth\")\n",
    "                        # OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n",
    "\n",
    "            cfg_save_output = f\"{OUTPUT_DIR}trained_tiny_model/lsg-electra-base/\"\n",
    "            \n",
    "            \n",
    "    # predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", \n",
    "    #                          map_location=torch.device('cpu'))['predictions']\n",
    "    predictions = torch.load(f\"{CFG.cfg_save_output}_fold{fold}_best.pth\", \n",
    "                             map_location=torch.device('cpu'))['predictions']\n",
    "    valid_folds[[f\"pred_{c}\" for c in CFG.target_cols]] = predictions\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return valid_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89c25bd8-5877-4a98-95ab-7594d158e189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lsg-electra-base'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG.model.split('/')[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dde3fb90-871c-4528-94ef-3437b8de1bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "LSGElectraConfig {\n",
      "  \"_name_or_path\": \"/root/autodl-tmp/lsg-electra-base/\",\n",
      "  \"adaptive\": true,\n",
      "  \"architectures\": [\n",
      "    \"LSGElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoModel\": \"modeling_lsg_electra.LSGElectraModel\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_lsg_electra.LSGElectraForCausalLM\",\n",
      "    \"AutoModelForMaskedLM\": \"modeling_lsg_electra.LSGElectraForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modeling_lsg_electra.LSGElectraForMultipleChoice\",\n",
      "    \"AutoModelForPreTraining\": \"modeling_lsg_electra.LSGElectraForPreTraining\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modeling_lsg_electra.LSGElectraForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modeling_lsg_electra.LSGElectraForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modeling_lsg_electra.LSGElectraForTokenClassification\"\n",
      "  },\n",
      "  \"base_model_prefix\": \"lsg\",\n",
      "  \"block_size\": 128,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"lsh_num_pre_rounds\": 1,\n",
      "  \"mask_first_token\": false,\n",
      "  \"max_position_embeddings\": 1536,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_global_tokens\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pool_with_global\": true,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sparse_block_size\": 128,\n",
      "  \"sparsity_factor\": 2,\n",
      "  \"sparsity_type\": \"norm\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Some weights of the model checkpoint at /root/autodl-tmp/lsg-electra-base/ were not used when initializing LSGElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing LSGElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LSGElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/366] Elapsed 0m 1s (remain 8m 37s) Loss: 2.4448(2.4448) Grad: inf  LR: 0.00002000  \n",
      "Epoch: [1][20/366] Elapsed 0m 7s (remain 2m 5s) Loss: 1.9463(2.1084) Grad: 213526.2656  LR: 0.00001999  \n",
      "Epoch: [1][40/366] Elapsed 0m 13s (remain 1m 45s) Loss: 0.3212(1.4827) Grad: 127912.7344  LR: 0.00001996  \n",
      "Epoch: [1][60/366] Elapsed 0m 19s (remain 1m 38s) Loss: 0.2921(1.0789) Grad: 112994.8750  LR: 0.00001991  \n",
      "Epoch: [1][80/366] Elapsed 0m 26s (remain 1m 32s) Loss: 0.2104(0.8637) Grad: 96628.5000  LR: 0.00001985  \n",
      "Epoch: [1][100/366] Elapsed 0m 32s (remain 1m 25s) Loss: 0.1454(0.7255) Grad: 125618.9844  LR: 0.00001977  \n",
      "Epoch: [1][120/366] Elapsed 0m 38s (remain 1m 18s) Loss: 0.2384(0.6324) Grad: 58255.9219  LR: 0.00001967  \n",
      "Epoch: [1][140/366] Elapsed 0m 45s (remain 1m 12s) Loss: 0.1064(0.5626) Grad: 49054.6484  LR: 0.00001955  \n",
      "Epoch: [1][160/366] Elapsed 0m 51s (remain 1m 5s) Loss: 0.1971(0.5096) Grad: 88765.9297  LR: 0.00001941  \n",
      "Epoch: [1][180/366] Elapsed 0m 57s (remain 0m 58s) Loss: 0.1991(0.4685) Grad: 154861.1094  LR: 0.00001926  \n",
      "Epoch: [1][200/366] Elapsed 1m 3s (remain 0m 52s) Loss: 0.1933(0.4349) Grad: 150417.2500  LR: 0.00001909  \n",
      "Epoch: [1][220/366] Elapsed 1m 9s (remain 0m 45s) Loss: 0.1497(0.4077) Grad: 152402.8750  LR: 0.00001890  \n",
      "Epoch: [1][240/366] Elapsed 1m 15s (remain 0m 39s) Loss: 0.1143(0.3847) Grad: 39914.8516  LR: 0.00001870  \n",
      "Epoch: [1][260/366] Elapsed 1m 21s (remain 0m 32s) Loss: 0.0922(0.3656) Grad: 64409.5078  LR: 0.00001848  \n",
      "Epoch: [1][280/366] Elapsed 1m 28s (remain 0m 26s) Loss: 0.0930(0.3484) Grad: 62804.5078  LR: 0.00001824  \n",
      "Epoch: [1][300/366] Elapsed 1m 34s (remain 0m 20s) Loss: 0.0761(0.3335) Grad: 40401.5430  LR: 0.00001799  \n",
      "Epoch: [1][320/366] Elapsed 1m 40s (remain 0m 14s) Loss: 0.1250(0.3221) Grad: 38270.1953  LR: 0.00001773  \n",
      "Epoch: [1][340/366] Elapsed 1m 46s (remain 0m 7s) Loss: 0.0970(0.3100) Grad: 38883.6016  LR: 0.00001745  \n",
      "Epoch: [1][360/366] Elapsed 1m 53s (remain 0m 1s) Loss: 0.0905(0.3000) Grad: 60385.2344  LR: 0.00001715  \n",
      "Epoch: [1][365/366] Elapsed 1m 54s (remain 0m 0s) Loss: 0.1306(0.2973) Grad: 116194.6016  LR: 0.00001708  \n",
      "EVAL: [0/62] Elapsed 0m 0s (remain 0m 42s) Loss: 0.1077(0.1077) \n",
      "EVAL: [20/62] Elapsed 0m 5s (remain 0m 10s) Loss: 0.0956(0.1209) \n",
      "EVAL: [40/62] Elapsed 0m 10s (remain 0m 5s) Loss: 0.1039(0.1190) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.2973  avg_val_loss: 0.1226  time: 130s\n",
      "Epoch 1 - Score: 0.4974  Scores: [0.5166656761352973, 0.4777127241669594, 0.47244990940846804, 0.4984803535383854, 0.5319704195334692, 0.4872868815061927]\n",
      "Epoch 1 - Save Best Score: 0.4974 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 15s (remain 0m 0s) Loss: 0.1134(0.1227) \n",
      "EVAL: [61/62] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0962(0.1226) \n",
      "Epoch: [2][0/366] Elapsed 0m 0s (remain 4m 24s) Loss: 0.1255(0.1255) Grad: 133688.5469  LR: 0.00001706  \n",
      "Epoch: [2][20/366] Elapsed 0m 7s (remain 1m 59s) Loss: 0.1093(0.1068) Grad: 93737.2578  LR: 0.00001675  \n",
      "Epoch: [2][40/366] Elapsed 0m 13s (remain 1m 48s) Loss: 0.1720(0.1177) Grad: 97595.2812  LR: 0.00001643  \n",
      "Epoch: [2][60/366] Elapsed 0m 19s (remain 1m 37s) Loss: 0.0870(0.1197) Grad: 106552.6797  LR: 0.00001610  \n",
      "Epoch: [2][80/366] Elapsed 0m 25s (remain 1m 29s) Loss: 0.1268(0.1220) Grad: 177491.2344  LR: 0.00001575  \n",
      "Epoch: [2][100/366] Elapsed 0m 31s (remain 1m 23s) Loss: 0.1473(0.1209) Grad: 208036.2188  LR: 0.00001540  \n",
      "Epoch: [2][120/366] Elapsed 0m 38s (remain 1m 17s) Loss: 0.0775(0.1198) Grad: 38357.0156  LR: 0.00001503  \n",
      "Epoch: [2][140/366] Elapsed 0m 44s (remain 1m 11s) Loss: 0.1905(0.1193) Grad: 133978.7031  LR: 0.00001466  \n",
      "Epoch: [2][160/366] Elapsed 0m 50s (remain 1m 4s) Loss: 0.0722(0.1176) Grad: 70488.8750  LR: 0.00001427  \n",
      "Epoch: [2][180/366] Elapsed 0m 57s (remain 0m 58s) Loss: 0.1311(0.1178) Grad: 203228.6406  LR: 0.00001388  \n",
      "Epoch: [2][200/366] Elapsed 1m 3s (remain 0m 51s) Loss: 0.0949(0.1175) Grad: 163936.6250  LR: 0.00001348  \n",
      "Epoch: [2][220/366] Elapsed 1m 9s (remain 0m 45s) Loss: 0.0969(0.1161) Grad: 167809.4531  LR: 0.00001308  \n",
      "Epoch: [2][240/366] Elapsed 1m 15s (remain 0m 39s) Loss: 0.1142(0.1158) Grad: 96521.8750  LR: 0.00001267  \n",
      "Epoch: [2][260/366] Elapsed 1m 22s (remain 0m 33s) Loss: 0.1299(0.1152) Grad: 73528.9922  LR: 0.00001225  \n",
      "Epoch: [2][280/366] Elapsed 1m 28s (remain 0m 26s) Loss: 0.1615(0.1149) Grad: 88171.6953  LR: 0.00001183  \n",
      "Epoch: [2][300/366] Elapsed 1m 35s (remain 0m 20s) Loss: 0.0873(0.1152) Grad: 113048.3359  LR: 0.00001141  \n",
      "Epoch: [2][320/366] Elapsed 1m 41s (remain 0m 14s) Loss: 0.1209(0.1150) Grad: 116394.1562  LR: 0.00001098  \n",
      "Epoch: [2][340/366] Elapsed 1m 47s (remain 0m 7s) Loss: 0.0601(0.1144) Grad: 84827.0391  LR: 0.00001056  \n",
      "Epoch: [2][360/366] Elapsed 1m 54s (remain 0m 1s) Loss: 0.1022(0.1146) Grad: 161141.4062  LR: 0.00001013  \n",
      "Epoch: [2][365/366] Elapsed 1m 56s (remain 0m 0s) Loss: 0.1177(0.1146) Grad: 64301.8672  LR: 0.00001002  \n",
      "EVAL: [0/62] Elapsed 0m 0s (remain 0m 42s) Loss: 0.0860(0.0860) \n",
      "EVAL: [20/62] Elapsed 0m 5s (remain 0m 10s) Loss: 0.0894(0.1101) \n",
      "EVAL: [40/62] Elapsed 0m 10s (remain 0m 5s) Loss: 0.0885(0.1093) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1146  avg_val_loss: 0.1122  time: 131s\n",
      "Epoch 2 - Score: 0.4749  Scores: [0.511003344931223, 0.46730272313591015, 0.42892431044116597, 0.47139263771343026, 0.5040661888428912, 0.46701069084870184]\n",
      "Epoch 2 - Save Best Score: 0.4749 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0968(0.1123) \n",
      "EVAL: [61/62] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0716(0.1122) \n",
      "Epoch: [3][0/366] Elapsed 0m 0s (remain 4m 51s) Loss: 0.0743(0.0743) Grad: 85734.0469  LR: 0.00001000  \n",
      "Epoch: [3][20/366] Elapsed 0m 7s (remain 1m 57s) Loss: 0.1263(0.0960) Grad: 151135.2031  LR: 0.00000957  \n",
      "Epoch: [3][40/366] Elapsed 0m 13s (remain 1m 46s) Loss: 0.1136(0.1026) Grad: 85873.7109  LR: 0.00000914  \n",
      "Epoch: [3][60/366] Elapsed 0m 19s (remain 1m 38s) Loss: 0.2070(0.1043) Grad: 131517.5312  LR: 0.00000872  \n",
      "Epoch: [3][80/366] Elapsed 0m 26s (remain 1m 31s) Loss: 0.1741(0.1053) Grad: 143399.0625  LR: 0.00000829  \n",
      "Epoch: [3][100/366] Elapsed 0m 32s (remain 1m 24s) Loss: 0.0712(0.1044) Grad: 86517.3594  LR: 0.00000787  \n",
      "Epoch: [3][120/366] Elapsed 0m 38s (remain 1m 17s) Loss: 0.0918(0.1061) Grad: 148512.7500  LR: 0.00000746  \n",
      "Epoch: [3][140/366] Elapsed 0m 44s (remain 1m 10s) Loss: 0.0913(0.1063) Grad: 91839.0781  LR: 0.00000704  \n",
      "Epoch: [3][160/366] Elapsed 0m 50s (remain 1m 4s) Loss: 0.0963(0.1053) Grad: 86378.8125  LR: 0.00000664  \n",
      "Epoch: [3][180/366] Elapsed 0m 56s (remain 0m 58s) Loss: 0.1045(0.1053) Grad: 116909.6406  LR: 0.00000624  \n",
      "Epoch: [3][200/366] Elapsed 1m 3s (remain 0m 52s) Loss: 0.1047(0.1043) Grad: 142373.4688  LR: 0.00000584  \n",
      "Epoch: [3][220/366] Elapsed 1m 9s (remain 0m 45s) Loss: 0.0891(0.1040) Grad: 97475.4531  LR: 0.00000546  \n",
      "Epoch: [3][240/366] Elapsed 1m 16s (remain 0m 39s) Loss: 0.1649(0.1040) Grad: 74053.0234  LR: 0.00000508  \n",
      "Epoch: [3][260/366] Elapsed 1m 22s (remain 0m 33s) Loss: 0.1036(0.1042) Grad: 63039.2891  LR: 0.00000471  \n",
      "Epoch: [3][280/366] Elapsed 1m 29s (remain 0m 26s) Loss: 0.0925(0.1040) Grad: 97663.9375  LR: 0.00000435  \n",
      "Epoch: [3][300/366] Elapsed 1m 35s (remain 0m 20s) Loss: 0.1388(0.1038) Grad: 175288.9219  LR: 0.00000400  \n",
      "Epoch: [3][320/366] Elapsed 1m 42s (remain 0m 14s) Loss: 0.0771(0.1037) Grad: 74988.1016  LR: 0.00000367  \n",
      "Epoch: [3][340/366] Elapsed 1m 48s (remain 0m 7s) Loss: 0.1164(0.1037) Grad: 109019.5938  LR: 0.00000334  \n",
      "Epoch: [3][360/366] Elapsed 1m 55s (remain 0m 1s) Loss: 0.0955(0.1032) Grad: 104508.4062  LR: 0.00000303  \n",
      "Epoch: [3][365/366] Elapsed 1m 56s (remain 0m 0s) Loss: 0.1052(0.1031) Grad: 85528.1875  LR: 0.00000295  \n",
      "EVAL: [0/62] Elapsed 0m 0s (remain 0m 41s) Loss: 0.0817(0.0817) \n",
      "EVAL: [20/62] Elapsed 0m 5s (remain 0m 10s) Loss: 0.0878(0.1060) \n",
      "EVAL: [40/62] Elapsed 0m 10s (remain 0m 5s) Loss: 0.0965(0.1064) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.1031  avg_val_loss: 0.1092  time: 132s\n",
      "Epoch 3 - Score: 0.4681  Scores: [0.5025967361055018, 0.46204271409425124, 0.42513572479434086, 0.46389033049285494, 0.4948173369103289, 0.4598998453924384]\n",
      "Epoch 3 - Save Best Score: 0.4681 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 15s (remain 0m 0s) Loss: 0.1001(0.1092) \n",
      "EVAL: [61/62] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0743(0.1092) \n",
      "Epoch: [4][0/366] Elapsed 0m 0s (remain 4m 57s) Loss: 0.0822(0.0822) Grad: 86965.6719  LR: 0.00000294  \n",
      "Epoch: [4][20/366] Elapsed 0m 7s (remain 2m 2s) Loss: 0.0919(0.1056) Grad: 222918.9688  LR: 0.00000264  \n",
      "Epoch: [4][40/366] Elapsed 0m 13s (remain 1m 50s) Loss: 0.0885(0.1041) Grad: 141148.1250  LR: 0.00000236  \n",
      "Epoch: [4][60/366] Elapsed 0m 20s (remain 1m 41s) Loss: 0.0871(0.1024) Grad: 67441.8984  LR: 0.00000209  \n",
      "Epoch: [4][80/366] Elapsed 0m 26s (remain 1m 34s) Loss: 0.0745(0.1020) Grad: 113434.6250  LR: 0.00000183  \n",
      "Epoch: [4][100/366] Elapsed 0m 32s (remain 1m 25s) Loss: 0.0694(0.1003) Grad: 62925.7070  LR: 0.00000159  \n",
      "Epoch: [4][120/366] Elapsed 0m 39s (remain 1m 19s) Loss: 0.0800(0.0983) Grad: 157641.1875  LR: 0.00000137  \n",
      "Epoch: [4][140/366] Elapsed 0m 45s (remain 1m 12s) Loss: 0.0868(0.0967) Grad: 132745.1875  LR: 0.00000116  \n",
      "Epoch: [4][160/366] Elapsed 0m 51s (remain 1m 5s) Loss: 0.0739(0.0960) Grad: 119930.9688  LR: 0.00000097  \n",
      "Epoch: [4][180/366] Elapsed 0m 57s (remain 0m 58s) Loss: 0.1073(0.0966) Grad: 67130.4141  LR: 0.00000079  \n",
      "Epoch: [4][200/366] Elapsed 1m 3s (remain 0m 52s) Loss: 0.0809(0.0961) Grad: 107680.7109  LR: 0.00000063  \n",
      "Epoch: [4][220/366] Elapsed 1m 9s (remain 0m 45s) Loss: 0.1278(0.0951) Grad: 124055.5547  LR: 0.00000049  \n",
      "Epoch: [4][240/366] Elapsed 1m 16s (remain 0m 39s) Loss: 0.0621(0.0943) Grad: 111739.9375  LR: 0.00000037  \n",
      "Epoch: [4][260/366] Elapsed 1m 22s (remain 0m 33s) Loss: 0.1035(0.0949) Grad: 78521.9141  LR: 0.00000026  \n",
      "Epoch: [4][280/366] Elapsed 1m 29s (remain 0m 26s) Loss: 0.0775(0.0946) Grad: 74231.2031  LR: 0.00000017  \n",
      "Epoch: [4][300/366] Elapsed 1m 35s (remain 0m 20s) Loss: 0.1259(0.0953) Grad: 132941.9688  LR: 0.00000010  \n",
      "Epoch: [4][320/366] Elapsed 1m 41s (remain 0m 14s) Loss: 0.0813(0.0951) Grad: 171325.8594  LR: 0.00000005  \n",
      "Epoch: [4][340/366] Elapsed 1m 48s (remain 0m 7s) Loss: 0.0816(0.0951) Grad: 90009.6016  LR: 0.00000002  \n",
      "Epoch: [4][360/366] Elapsed 1m 53s (remain 0m 1s) Loss: 0.0633(0.0949) Grad: 75652.1016  LR: 0.00000000  \n",
      "Epoch: [4][365/366] Elapsed 1m 55s (remain 0m 0s) Loss: 0.0636(0.0946) Grad: 74071.5625  LR: 0.00000000  \n",
      "EVAL: [0/62] Elapsed 0m 0s (remain 0m 42s) Loss: 0.0805(0.0805) \n",
      "EVAL: [20/62] Elapsed 0m 5s (remain 0m 10s) Loss: 0.0875(0.1073) \n",
      "EVAL: [40/62] Elapsed 0m 10s (remain 0m 5s) Loss: 0.0943(0.1074) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0946  avg_val_loss: 0.1098  time: 131s\n",
      "Epoch 4 - Score: 0.4696  Scores: [0.5064835694830998, 0.4665849365219889, 0.42551874960821384, 0.46488545180786944, 0.4940782801577294, 0.4598645192609039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0946(0.1099) \n",
      "EVAL: [61/62] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0592(0.1098) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 result ==========\n",
      "Score: 0.4681  Scores: [0.5025967361055018, 0.46204271409425124, 0.42513572479434086, 0.46389033049285494, 0.4948173369103289, 0.4598998453924384]\n",
      "========== fold: 1 training ==========\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "LSGElectraConfig {\n",
      "  \"_name_or_path\": \"/root/autodl-tmp/lsg-electra-base/\",\n",
      "  \"adaptive\": true,\n",
      "  \"architectures\": [\n",
      "    \"LSGElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoModel\": \"modeling_lsg_electra.LSGElectraModel\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_lsg_electra.LSGElectraForCausalLM\",\n",
      "    \"AutoModelForMaskedLM\": \"modeling_lsg_electra.LSGElectraForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modeling_lsg_electra.LSGElectraForMultipleChoice\",\n",
      "    \"AutoModelForPreTraining\": \"modeling_lsg_electra.LSGElectraForPreTraining\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modeling_lsg_electra.LSGElectraForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modeling_lsg_electra.LSGElectraForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modeling_lsg_electra.LSGElectraForTokenClassification\"\n",
      "  },\n",
      "  \"base_model_prefix\": \"lsg\",\n",
      "  \"block_size\": 128,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"lsh_num_pre_rounds\": 1,\n",
      "  \"mask_first_token\": false,\n",
      "  \"max_position_embeddings\": 1536,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_global_tokens\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pool_with_global\": true,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sparse_block_size\": 128,\n",
      "  \"sparsity_factor\": 2,\n",
      "  \"sparsity_type\": \"norm\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Some weights of the model checkpoint at /root/autodl-tmp/lsg-electra-base/ were not used when initializing LSGElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing LSGElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LSGElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/366] Elapsed 0m 0s (remain 4m 48s) Loss: 2.2501(2.2501) Grad: inf  LR: 0.00002000  \n",
      "Epoch: [1][20/366] Elapsed 0m 7s (remain 2m 6s) Loss: 1.3879(2.1837) Grad: 212690.9844  LR: 0.00001999  \n",
      "Epoch: [1][40/366] Elapsed 0m 13s (remain 1m 45s) Loss: 0.2526(1.4402) Grad: 153389.8750  LR: 0.00001996  \n",
      "Epoch: [1][60/366] Elapsed 0m 19s (remain 1m 38s) Loss: 0.1183(1.0451) Grad: 56462.6328  LR: 0.00001991  \n",
      "Epoch: [1][80/366] Elapsed 0m 25s (remain 1m 31s) Loss: 0.1894(0.8329) Grad: 288352.5625  LR: 0.00001985  \n",
      "Epoch: [1][100/366] Elapsed 0m 31s (remain 1m 23s) Loss: 0.1948(0.7084) Grad: 112787.1016  LR: 0.00001977  \n",
      "Epoch: [1][120/366] Elapsed 0m 37s (remain 1m 16s) Loss: 0.0865(0.6178) Grad: 58513.7227  LR: 0.00001967  \n",
      "Epoch: [1][140/366] Elapsed 0m 44s (remain 1m 10s) Loss: 0.0692(0.5510) Grad: 72905.6562  LR: 0.00001955  \n",
      "Epoch: [1][160/366] Elapsed 0m 50s (remain 1m 4s) Loss: 0.1182(0.5017) Grad: 90096.5391  LR: 0.00001941  \n",
      "Epoch: [1][180/366] Elapsed 0m 56s (remain 0m 58s) Loss: 0.2530(0.4615) Grad: 100820.6094  LR: 0.00001926  \n",
      "Epoch: [1][200/366] Elapsed 1m 3s (remain 0m 52s) Loss: 0.1228(0.4285) Grad: 36043.5039  LR: 0.00001909  \n",
      "Epoch: [1][220/366] Elapsed 1m 9s (remain 0m 45s) Loss: 0.1036(0.4035) Grad: 29589.3789  LR: 0.00001890  \n",
      "Epoch: [1][240/366] Elapsed 1m 16s (remain 0m 39s) Loss: 0.1354(0.3833) Grad: 71515.8203  LR: 0.00001870  \n",
      "Epoch: [1][260/366] Elapsed 1m 22s (remain 0m 33s) Loss: 0.1337(0.3644) Grad: 107321.8750  LR: 0.00001848  \n",
      "Epoch: [1][280/366] Elapsed 1m 29s (remain 0m 26s) Loss: 0.1254(0.3496) Grad: 68241.3516  LR: 0.00001824  \n",
      "Epoch: [1][300/366] Elapsed 1m 35s (remain 0m 20s) Loss: 0.1539(0.3346) Grad: 46538.5703  LR: 0.00001799  \n",
      "Epoch: [1][320/366] Elapsed 1m 41s (remain 0m 14s) Loss: 0.0906(0.3218) Grad: 40536.0430  LR: 0.00001773  \n",
      "Epoch: [1][340/366] Elapsed 1m 47s (remain 0m 7s) Loss: 0.1707(0.3109) Grad: 122382.2188  LR: 0.00001745  \n",
      "Epoch: [1][360/366] Elapsed 1m 53s (remain 0m 1s) Loss: 0.1089(0.3007) Grad: 86880.1094  LR: 0.00001716  \n",
      "Epoch: [1][365/366] Elapsed 1m 55s (remain 0m 0s) Loss: 0.0763(0.2988) Grad: 48350.2891  LR: 0.00001708  \n",
      "EVAL: [0/62] Elapsed 0m 0s (remain 0m 40s) Loss: 0.1023(0.1023) \n",
      "EVAL: [20/62] Elapsed 0m 5s (remain 0m 10s) Loss: 0.1487(0.1350) \n",
      "EVAL: [40/62] Elapsed 0m 10s (remain 0m 5s) Loss: 0.1075(0.1335) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.2988  avg_val_loss: 0.1304  time: 131s\n",
      "Epoch 1 - Score: 0.5130  Scores: [0.55899716385439, 0.49153571409377617, 0.459647995662612, 0.5031437403149736, 0.5412977615601366, 0.5233889810335599]\n",
      "Epoch 1 - Save Best Score: 0.5130 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 15s (remain 0m 0s) Loss: 0.1119(0.1305) \n",
      "EVAL: [61/62] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0552(0.1304) \n",
      "Epoch: [2][0/366] Elapsed 0m 0s (remain 5m 7s) Loss: 0.0915(0.0915) Grad: 96831.8906  LR: 0.00001707  \n",
      "Epoch: [2][20/366] Elapsed 0m 6s (remain 1m 52s) Loss: 0.1981(0.1318) Grad: 284675.3750  LR: 0.00001676  \n",
      "Epoch: [2][40/366] Elapsed 0m 13s (remain 1m 43s) Loss: 0.1599(0.1282) Grad: 58814.6523  LR: 0.00001644  \n",
      "Epoch: [2][60/366] Elapsed 0m 18s (remain 1m 34s) Loss: 0.1287(0.1230) Grad: 97758.0703  LR: 0.00001610  \n",
      "Epoch: [2][80/366] Elapsed 0m 25s (remain 1m 29s) Loss: 0.1320(0.1219) Grad: 208734.0781  LR: 0.00001576  \n",
      "Epoch: [2][100/366] Elapsed 0m 31s (remain 1m 22s) Loss: 0.0987(0.1196) Grad: 73660.2422  LR: 0.00001540  \n",
      "Epoch: [2][120/366] Elapsed 0m 37s (remain 1m 16s) Loss: 0.1122(0.1178) Grad: 86569.4609  LR: 0.00001504  \n",
      "Epoch: [2][140/366] Elapsed 0m 44s (remain 1m 10s) Loss: 0.1278(0.1176) Grad: 141645.9844  LR: 0.00001466  \n",
      "Epoch: [2][160/366] Elapsed 0m 50s (remain 1m 4s) Loss: 0.1215(0.1177) Grad: 138153.7969  LR: 0.00001428  \n",
      "Epoch: [2][180/366] Elapsed 0m 57s (remain 0m 58s) Loss: 0.1351(0.1184) Grad: 79632.9609  LR: 0.00001389  \n",
      "Epoch: [2][200/366] Elapsed 1m 3s (remain 0m 52s) Loss: 0.1217(0.1179) Grad: 99400.7891  LR: 0.00001349  \n",
      "Epoch: [2][220/366] Elapsed 1m 10s (remain 0m 46s) Loss: 0.0668(0.1171) Grad: 41456.0898  LR: 0.00001309  \n",
      "Epoch: [2][240/366] Elapsed 1m 16s (remain 0m 39s) Loss: 0.0811(0.1162) Grad: 103166.9766  LR: 0.00001268  \n",
      "Epoch: [2][260/366] Elapsed 1m 22s (remain 0m 33s) Loss: 0.0974(0.1168) Grad: 149870.5469  LR: 0.00001226  \n",
      "Epoch: [2][280/366] Elapsed 1m 29s (remain 0m 27s) Loss: 0.0938(0.1159) Grad: 62851.6719  LR: 0.00001184  \n",
      "Epoch: [2][300/366] Elapsed 1m 35s (remain 0m 20s) Loss: 0.0912(0.1159) Grad: 105425.5547  LR: 0.00001142  \n",
      "Epoch: [2][320/366] Elapsed 1m 42s (remain 0m 14s) Loss: 0.1370(0.1161) Grad: 96923.0625  LR: 0.00001099  \n",
      "Epoch: [2][340/366] Elapsed 1m 48s (remain 0m 7s) Loss: 0.0933(0.1154) Grad: 112238.3828  LR: 0.00001057  \n",
      "Epoch: [2][360/366] Elapsed 1m 54s (remain 0m 1s) Loss: 0.1416(0.1156) Grad: 173711.6875  LR: 0.00001014  \n",
      "Epoch: [2][365/366] Elapsed 1m 56s (remain 0m 0s) Loss: 0.1198(0.1157) Grad: 140924.2969  LR: 0.00001003  \n",
      "EVAL: [0/62] Elapsed 0m 0s (remain 0m 43s) Loss: 0.0959(0.0959) \n",
      "EVAL: [20/62] Elapsed 0m 5s (remain 0m 10s) Loss: 0.1281(0.1206) \n",
      "EVAL: [40/62] Elapsed 0m 10s (remain 0m 5s) Loss: 0.1155(0.1199) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1157  avg_val_loss: 0.1178  time: 132s\n",
      "Epoch 2 - Score: 0.4869  Scores: [0.5200962473484135, 0.48398211077820025, 0.4460111846590913, 0.4744885872705223, 0.5109707032588287, 0.48599647081610525]\n",
      "Epoch 2 - Save Best Score: 0.4869 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 15s (remain 0m 0s) Loss: 0.1118(0.1179) \n",
      "EVAL: [61/62] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0576(0.1178) \n",
      "Epoch: [3][0/366] Elapsed 0m 0s (remain 4m 53s) Loss: 0.1151(0.1151) Grad: 110248.5625  LR: 0.00001001  \n",
      "Epoch: [3][20/366] Elapsed 0m 6s (remain 1m 54s) Loss: 0.1212(0.1183) Grad: 118035.5156  LR: 0.00000958  \n",
      "Epoch: [3][40/366] Elapsed 0m 13s (remain 1m 46s) Loss: 0.1091(0.1124) Grad: 202701.0781  LR: 0.00000916  \n",
      "Epoch: [3][60/366] Elapsed 0m 19s (remain 1m 38s) Loss: 0.0761(0.1073) Grad: 110129.1484  LR: 0.00000873  \n",
      "Epoch: [3][80/366] Elapsed 0m 26s (remain 1m 31s) Loss: 0.0899(0.1039) Grad: 171845.8125  LR: 0.00000831  \n",
      "Epoch: [3][100/366] Elapsed 0m 32s (remain 1m 24s) Loss: 0.0987(0.1036) Grad: 143206.3750  LR: 0.00000789  \n",
      "Epoch: [3][120/366] Elapsed 0m 38s (remain 1m 17s) Loss: 0.0697(0.1041) Grad: 116529.3672  LR: 0.00000747  \n",
      "Epoch: [3][140/366] Elapsed 0m 44s (remain 1m 11s) Loss: 0.0707(0.1053) Grad: 63406.2031  LR: 0.00000706  \n",
      "Epoch: [3][160/366] Elapsed 0m 50s (remain 1m 4s) Loss: 0.1424(0.1056) Grad: 113842.9062  LR: 0.00000665  \n",
      "Epoch: [3][180/366] Elapsed 0m 56s (remain 0m 57s) Loss: 0.1273(0.1056) Grad: 146214.2188  LR: 0.00000625  \n",
      "Epoch: [3][200/366] Elapsed 1m 2s (remain 0m 51s) Loss: 0.1361(0.1045) Grad: 91418.7109  LR: 0.00000586  \n",
      "Epoch: [3][220/366] Elapsed 1m 9s (remain 0m 45s) Loss: 0.0758(0.1038) Grad: 95857.5938  LR: 0.00000547  \n",
      "Epoch: [3][240/366] Elapsed 1m 15s (remain 0m 39s) Loss: 0.0871(0.1033) Grad: 211411.8281  LR: 0.00000509  \n",
      "Epoch: [3][260/366] Elapsed 1m 21s (remain 0m 32s) Loss: 0.0903(0.1028) Grad: 134736.4844  LR: 0.00000472  \n",
      "Epoch: [3][280/366] Elapsed 1m 28s (remain 0m 26s) Loss: 0.0878(0.1031) Grad: 108576.1953  LR: 0.00000437  \n",
      "Epoch: [3][300/366] Elapsed 1m 34s (remain 0m 20s) Loss: 0.0678(0.1026) Grad: 108518.9062  LR: 0.00000402  \n",
      "Epoch: [3][320/366] Elapsed 1m 41s (remain 0m 14s) Loss: 0.0734(0.1025) Grad: 165805.3281  LR: 0.00000368  \n",
      "Epoch: [3][340/366] Elapsed 1m 47s (remain 0m 7s) Loss: 0.1269(0.1028) Grad: 115372.3672  LR: 0.00000335  \n",
      "Epoch: [3][360/366] Elapsed 1m 54s (remain 0m 1s) Loss: 0.1114(0.1031) Grad: 101921.3203  LR: 0.00000304  \n",
      "Epoch: [3][365/366] Elapsed 1m 55s (remain 0m 0s) Loss: 0.0718(0.1029) Grad: 118015.1094  LR: 0.00000296  \n",
      "EVAL: [0/62] Elapsed 0m 0s (remain 0m 41s) Loss: 0.0921(0.0921) \n",
      "EVAL: [20/62] Elapsed 0m 5s (remain 0m 10s) Loss: 0.1211(0.1172) \n",
      "EVAL: [40/62] Elapsed 0m 10s (remain 0m 5s) Loss: 0.1142(0.1155) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.1029  avg_val_loss: 0.1126  time: 131s\n",
      "Epoch 3 - Score: 0.4757  Scores: [0.5093139902830198, 0.46944658411694795, 0.43352235939229966, 0.469144937097953, 0.5007311558349079, 0.4718798646166392]\n",
      "Epoch 3 - Save Best Score: 0.4757 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 15s (remain 0m 0s) Loss: 0.1085(0.1127) \n",
      "EVAL: [61/62] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0543(0.1126) \n",
      "Epoch: [4][0/366] Elapsed 0m 0s (remain 4m 34s) Loss: 0.0597(0.0597) Grad: 104740.5391  LR: 0.00000295  \n",
      "Epoch: [4][20/366] Elapsed 0m 7s (remain 1m 58s) Loss: 0.0964(0.0912) Grad: 101348.9141  LR: 0.00000265  \n",
      "Epoch: [4][40/366] Elapsed 0m 13s (remain 1m 44s) Loss: 0.1084(0.0925) Grad: 213679.0625  LR: 0.00000237  \n",
      "Epoch: [4][60/366] Elapsed 0m 19s (remain 1m 37s) Loss: 0.1003(0.0936) Grad: 137094.1875  LR: 0.00000210  \n",
      "Epoch: [4][80/366] Elapsed 0m 26s (remain 1m 33s) Loss: 0.0851(0.0952) Grad: 61194.8320  LR: 0.00000184  \n",
      "Epoch: [4][100/366] Elapsed 0m 33s (remain 1m 26s) Loss: 0.0834(0.0960) Grad: 71062.8359  LR: 0.00000160  \n",
      "Epoch: [4][120/366] Elapsed 0m 38s (remain 1m 18s) Loss: 0.0521(0.0947) Grad: 62318.8477  LR: 0.00000138  \n",
      "Epoch: [4][140/366] Elapsed 0m 45s (remain 1m 12s) Loss: 0.1133(0.0955) Grad: 68697.4609  LR: 0.00000117  \n",
      "Epoch: [4][160/366] Elapsed 0m 51s (remain 1m 5s) Loss: 0.1763(0.0966) Grad: 127271.5312  LR: 0.00000098  \n",
      "Epoch: [4][180/366] Elapsed 0m 57s (remain 0m 58s) Loss: 0.0890(0.0956) Grad: 67393.2109  LR: 0.00000080  \n",
      "Epoch: [4][200/366] Elapsed 1m 4s (remain 0m 52s) Loss: 0.1970(0.0961) Grad: 180462.4219  LR: 0.00000064  \n",
      "Epoch: [4][220/366] Elapsed 1m 10s (remain 0m 46s) Loss: 0.1243(0.0971) Grad: 97129.2812  LR: 0.00000050  \n",
      "Epoch: [4][240/366] Elapsed 1m 17s (remain 0m 39s) Loss: 0.0757(0.0967) Grad: 71457.9609  LR: 0.00000037  \n",
      "Epoch: [4][260/366] Elapsed 1m 23s (remain 0m 33s) Loss: 0.0971(0.0963) Grad: 97078.6328  LR: 0.00000027  \n",
      "Epoch: [4][280/366] Elapsed 1m 28s (remain 0m 26s) Loss: 0.1007(0.0957) Grad: 220963.4531  LR: 0.00000018  \n",
      "Epoch: [4][300/366] Elapsed 1m 35s (remain 0m 20s) Loss: 0.0921(0.0957) Grad: 79172.3281  LR: 0.00000011  \n",
      "Epoch: [4][320/366] Elapsed 1m 41s (remain 0m 14s) Loss: 0.1286(0.0959) Grad: 151618.7656  LR: 0.00000005  \n",
      "Epoch: [4][340/366] Elapsed 1m 48s (remain 0m 7s) Loss: 0.0979(0.0960) Grad: 99590.2422  LR: 0.00000002  \n",
      "Epoch: [4][360/366] Elapsed 1m 54s (remain 0m 1s) Loss: 0.0769(0.0961) Grad: 93656.0156  LR: 0.00000000  \n",
      "Epoch: [4][365/366] Elapsed 1m 56s (remain 0m 0s) Loss: 0.0745(0.0959) Grad: 70291.1641  LR: 0.00000000  \n",
      "EVAL: [0/62] Elapsed 0m 0s (remain 0m 40s) Loss: 0.0932(0.0932) \n",
      "EVAL: [20/62] Elapsed 0m 5s (remain 0m 10s) Loss: 0.1169(0.1171) \n",
      "EVAL: [40/62] Elapsed 0m 10s (remain 0m 5s) Loss: 0.1115(0.1154) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0959  avg_val_loss: 0.1131  time: 132s\n",
      "Epoch 4 - Score: 0.4767  Scores: [0.5110707541429977, 0.47154084485214914, 0.4320079536237206, 0.4714024476256716, 0.5013375726240118, 0.47254639172551777]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 15s (remain 0m 0s) Loss: 0.1112(0.1132) \n",
      "EVAL: [61/62] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0512(0.1131) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 1 result ==========\n",
      "Score: 0.4757  Scores: [0.5093139902830198, 0.46944658411694795, 0.43352235939229966, 0.469144937097953, 0.5007311558349079, 0.4718798646166392]\n",
      "========== fold: 2 training ==========\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "LSGElectraConfig {\n",
      "  \"_name_or_path\": \"/root/autodl-tmp/lsg-electra-base/\",\n",
      "  \"adaptive\": true,\n",
      "  \"architectures\": [\n",
      "    \"LSGElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoModel\": \"modeling_lsg_electra.LSGElectraModel\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_lsg_electra.LSGElectraForCausalLM\",\n",
      "    \"AutoModelForMaskedLM\": \"modeling_lsg_electra.LSGElectraForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modeling_lsg_electra.LSGElectraForMultipleChoice\",\n",
      "    \"AutoModelForPreTraining\": \"modeling_lsg_electra.LSGElectraForPreTraining\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modeling_lsg_electra.LSGElectraForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modeling_lsg_electra.LSGElectraForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modeling_lsg_electra.LSGElectraForTokenClassification\"\n",
      "  },\n",
      "  \"base_model_prefix\": \"lsg\",\n",
      "  \"block_size\": 128,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"lsh_num_pre_rounds\": 1,\n",
      "  \"mask_first_token\": false,\n",
      "  \"max_position_embeddings\": 1536,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_global_tokens\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pool_with_global\": true,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sparse_block_size\": 128,\n",
      "  \"sparsity_factor\": 2,\n",
      "  \"sparsity_type\": \"norm\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Some weights of the model checkpoint at /root/autodl-tmp/lsg-electra-base/ were not used when initializing LSGElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing LSGElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LSGElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/366] Elapsed 0m 0s (remain 4m 52s) Loss: 2.7859(2.7859) Grad: inf  LR: 0.00002000  \n",
      "Epoch: [1][20/366] Elapsed 0m 7s (remain 1m 55s) Loss: 1.9210(2.1938) Grad: 223487.8125  LR: 0.00001999  \n",
      "Epoch: [1][40/366] Elapsed 0m 13s (remain 1m 46s) Loss: 0.3617(1.5445) Grad: 104310.2031  LR: 0.00001996  \n",
      "Epoch: [1][60/366] Elapsed 0m 20s (remain 1m 42s) Loss: 0.1530(1.1192) Grad: 48284.1133  LR: 0.00001991  \n",
      "Epoch: [1][80/366] Elapsed 0m 26s (remain 1m 34s) Loss: 0.1333(0.8976) Grad: 33126.9688  LR: 0.00001985  \n",
      "Epoch: [1][100/366] Elapsed 0m 33s (remain 1m 27s) Loss: 0.2422(0.7586) Grad: 160463.2188  LR: 0.00001977  \n",
      "Epoch: [1][120/366] Elapsed 0m 39s (remain 1m 20s) Loss: 0.1587(0.6689) Grad: 51797.6758  LR: 0.00001967  \n",
      "Epoch: [1][140/366] Elapsed 0m 45s (remain 1m 12s) Loss: 0.1443(0.6031) Grad: 53282.5742  LR: 0.00001955  \n",
      "Epoch: [1][160/366] Elapsed 0m 51s (remain 1m 5s) Loss: 0.2455(0.5504) Grad: 48908.0273  LR: 0.00001941  \n",
      "Epoch: [1][180/366] Elapsed 0m 58s (remain 0m 59s) Loss: 0.2647(0.5051) Grad: 179611.0156  LR: 0.00001926  \n",
      "Epoch: [1][200/366] Elapsed 1m 4s (remain 0m 52s) Loss: 0.1282(0.4709) Grad: 126197.7891  LR: 0.00001909  \n",
      "Epoch: [1][220/366] Elapsed 1m 10s (remain 0m 46s) Loss: 0.0858(0.4418) Grad: 44075.5039  LR: 0.00001890  \n",
      "Epoch: [1][240/366] Elapsed 1m 17s (remain 0m 40s) Loss: 0.1297(0.4161) Grad: 88965.5156  LR: 0.00001870  \n",
      "Epoch: [1][260/366] Elapsed 1m 23s (remain 0m 33s) Loss: 0.1193(0.3946) Grad: 48891.9688  LR: 0.00001848  \n",
      "Epoch: [1][280/366] Elapsed 1m 30s (remain 0m 27s) Loss: 0.1267(0.3762) Grad: 51189.1250  LR: 0.00001824  \n",
      "Epoch: [1][300/366] Elapsed 1m 36s (remain 0m 20s) Loss: 0.0754(0.3602) Grad: 59198.9883  LR: 0.00001799  \n",
      "Epoch: [1][320/366] Elapsed 1m 42s (remain 0m 14s) Loss: 0.2626(0.3458) Grad: 127367.0312  LR: 0.00001773  \n",
      "Epoch: [1][340/366] Elapsed 1m 49s (remain 0m 8s) Loss: 0.1186(0.3339) Grad: 65325.2773  LR: 0.00001745  \n",
      "Epoch: [1][360/366] Elapsed 1m 56s (remain 0m 1s) Loss: 0.1811(0.3229) Grad: 67002.1016  LR: 0.00001715  \n",
      "Epoch: [1][365/366] Elapsed 1m 57s (remain 0m 0s) Loss: 0.1126(0.3202) Grad: 85163.7969  LR: 0.00001708  \n",
      "EVAL: [0/62] Elapsed 0m 0s (remain 0m 50s) Loss: 0.1290(0.1290) \n",
      "EVAL: [20/62] Elapsed 0m 5s (remain 0m 10s) Loss: 0.1232(0.1283) \n",
      "EVAL: [40/62] Elapsed 0m 9s (remain 0m 4s) Loss: 0.1098(0.1323) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.3202  avg_val_loss: 0.1305  time: 132s\n",
      "Epoch 1 - Score: 0.5140  Scores: [0.5231948509319471, 0.4789838806558671, 0.4850892884652619, 0.504153810548069, 0.5466567238066969, 0.5459839602892322]\n",
      "Epoch 1 - Save Best Score: 0.5140 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 14s (remain 0m 0s) Loss: 0.1068(0.1302) \n",
      "EVAL: [61/62] Elapsed 0m 14s (remain 0m 0s) Loss: 0.2796(0.1305) \n",
      "Epoch: [2][0/366] Elapsed 0m 0s (remain 5m 51s) Loss: 0.1003(0.1003) Grad: 99603.9141  LR: 0.00001706  \n",
      "Epoch: [2][20/366] Elapsed 0m 7s (remain 2m 1s) Loss: 0.1049(0.1111) Grad: 56966.7578  LR: 0.00001675  \n",
      "Epoch: [2][40/366] Elapsed 0m 13s (remain 1m 48s) Loss: 0.1053(0.1139) Grad: 75339.7656  LR: 0.00001643  \n",
      "Epoch: [2][60/366] Elapsed 0m 20s (remain 1m 40s) Loss: 0.1382(0.1151) Grad: 173223.5156  LR: 0.00001610  \n",
      "Epoch: [2][80/366] Elapsed 0m 25s (remain 1m 31s) Loss: 0.0992(0.1116) Grad: 114574.9297  LR: 0.00001575  \n",
      "Epoch: [2][100/366] Elapsed 0m 32s (remain 1m 25s) Loss: 0.1101(0.1123) Grad: 89592.1406  LR: 0.00001540  \n",
      "Epoch: [2][120/366] Elapsed 0m 39s (remain 1m 19s) Loss: 0.0803(0.1143) Grad: 80803.4688  LR: 0.00001503  \n",
      "Epoch: [2][140/366] Elapsed 0m 46s (remain 1m 13s) Loss: 0.2108(0.1164) Grad: 120474.0391  LR: 0.00001466  \n",
      "Epoch: [2][160/366] Elapsed 0m 52s (remain 1m 6s) Loss: 0.1301(0.1168) Grad: 173708.7188  LR: 0.00001427  \n",
      "Epoch: [2][180/366] Elapsed 0m 59s (remain 1m 0s) Loss: 0.1340(0.1170) Grad: 121517.7266  LR: 0.00001388  \n",
      "Epoch: [2][200/366] Elapsed 1m 5s (remain 0m 54s) Loss: 0.1471(0.1162) Grad: 169039.1875  LR: 0.00001348  \n",
      "Epoch: [2][220/366] Elapsed 1m 12s (remain 0m 47s) Loss: 0.1230(0.1162) Grad: 143822.7500  LR: 0.00001308  \n",
      "Epoch: [2][240/366] Elapsed 1m 18s (remain 0m 40s) Loss: 0.0764(0.1171) Grad: 93481.5391  LR: 0.00001267  \n",
      "Epoch: [2][260/366] Elapsed 1m 25s (remain 0m 34s) Loss: 0.1801(0.1176) Grad: 155926.1719  LR: 0.00001225  \n",
      "Epoch: [2][280/366] Elapsed 1m 31s (remain 0m 27s) Loss: 0.1054(0.1178) Grad: 85837.7656  LR: 0.00001183  \n",
      "Epoch: [2][300/366] Elapsed 1m 37s (remain 0m 21s) Loss: 0.1125(0.1166) Grad: 132052.4062  LR: 0.00001141  \n",
      "Epoch: [2][320/366] Elapsed 1m 43s (remain 0m 14s) Loss: 0.1064(0.1162) Grad: 93293.2578  LR: 0.00001098  \n",
      "Epoch: [2][340/366] Elapsed 1m 49s (remain 0m 8s) Loss: 0.0644(0.1155) Grad: 60317.0000  LR: 0.00001056  \n",
      "Epoch: [2][360/366] Elapsed 1m 56s (remain 0m 1s) Loss: 0.0655(0.1146) Grad: 79884.4609  LR: 0.00001013  \n",
      "Epoch: [2][365/366] Elapsed 1m 57s (remain 0m 0s) Loss: 0.1500(0.1146) Grad: 240423.5625  LR: 0.00001002  \n",
      "EVAL: [0/62] Elapsed 0m 0s (remain 0m 47s) Loss: 0.1263(0.1263) \n",
      "EVAL: [20/62] Elapsed 0m 5s (remain 0m 10s) Loss: 0.0792(0.1201) \n",
      "EVAL: [40/62] Elapsed 0m 9s (remain 0m 4s) Loss: 0.0937(0.1217) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1146  avg_val_loss: 0.1209  time: 132s\n",
      "Epoch 2 - Score: 0.4945  Scores: [0.5291488393341985, 0.4784461386015409, 0.4646683124384198, 0.5023636445398333, 0.4965168905174975, 0.49564576305334257]\n",
      "Epoch 2 - Save Best Score: 0.4945 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 14s (remain 0m 0s) Loss: 0.0998(0.1209) \n",
      "EVAL: [61/62] Elapsed 0m 14s (remain 0m 0s) Loss: 0.1308(0.1209) \n",
      "Epoch: [3][0/366] Elapsed 0m 0s (remain 5m 11s) Loss: 0.0763(0.0763) Grad: 45627.6250  LR: 0.00001000  \n",
      "Epoch: [3][20/366] Elapsed 0m 7s (remain 1m 59s) Loss: 0.1329(0.1053) Grad: 243299.3125  LR: 0.00000957  \n",
      "Epoch: [3][40/366] Elapsed 0m 13s (remain 1m 47s) Loss: 0.1435(0.1035) Grad: 218939.3281  LR: 0.00000914  \n",
      "Epoch: [3][60/366] Elapsed 0m 19s (remain 1m 36s) Loss: 0.0604(0.1029) Grad: 84567.7812  LR: 0.00000872  \n",
      "Epoch: [3][80/366] Elapsed 0m 25s (remain 1m 31s) Loss: 0.0947(0.0999) Grad: 75751.9141  LR: 0.00000829  \n",
      "Epoch: [3][100/366] Elapsed 0m 31s (remain 1m 23s) Loss: 0.1100(0.0993) Grad: 69439.2422  LR: 0.00000787  \n",
      "Epoch: [3][120/366] Elapsed 0m 37s (remain 1m 16s) Loss: 0.1047(0.0995) Grad: 151694.9844  LR: 0.00000746  \n",
      "Epoch: [3][140/366] Elapsed 0m 44s (remain 1m 10s) Loss: 0.1320(0.1002) Grad: 190952.0156  LR: 0.00000704  \n",
      "Epoch: [3][160/366] Elapsed 0m 50s (remain 1m 4s) Loss: 0.1172(0.1026) Grad: 86692.4766  LR: 0.00000664  \n",
      "Epoch: [3][180/366] Elapsed 0m 57s (remain 0m 59s) Loss: 0.1761(0.1038) Grad: 100382.3906  LR: 0.00000624  \n",
      "Epoch: [3][200/366] Elapsed 1m 4s (remain 0m 52s) Loss: 0.0754(0.1035) Grad: 136050.3438  LR: 0.00000584  \n",
      "Epoch: [3][220/366] Elapsed 1m 10s (remain 0m 46s) Loss: 0.1706(0.1033) Grad: 151382.0938  LR: 0.00000546  \n",
      "Epoch: [3][240/366] Elapsed 1m 17s (remain 0m 40s) Loss: 0.0599(0.1028) Grad: 62450.9062  LR: 0.00000508  \n",
      "Epoch: [3][260/366] Elapsed 1m 24s (remain 0m 33s) Loss: 0.0788(0.1029) Grad: 112953.6250  LR: 0.00000471  \n",
      "Epoch: [3][280/366] Elapsed 1m 30s (remain 0m 27s) Loss: 0.0904(0.1019) Grad: 78370.0156  LR: 0.00000435  \n",
      "Epoch: [3][300/366] Elapsed 1m 37s (remain 0m 20s) Loss: 0.0842(0.1025) Grad: 108665.7656  LR: 0.00000400  \n",
      "Epoch: [3][320/366] Elapsed 1m 43s (remain 0m 14s) Loss: 0.0957(0.1025) Grad: 75500.3047  LR: 0.00000367  \n",
      "Epoch: [3][340/366] Elapsed 1m 49s (remain 0m 8s) Loss: 0.2061(0.1023) Grad: 210091.1250  LR: 0.00000334  \n",
      "Epoch: [3][360/366] Elapsed 1m 56s (remain 0m 1s) Loss: 0.1097(0.1026) Grad: 121604.6094  LR: 0.00000303  \n",
      "Epoch: [3][365/366] Elapsed 1m 57s (remain 0m 0s) Loss: 0.1259(0.1026) Grad: 122069.2812  LR: 0.00000295  \n",
      "EVAL: [0/62] Elapsed 0m 0s (remain 0m 47s) Loss: 0.1258(0.1258) \n",
      "EVAL: [20/62] Elapsed 0m 5s (remain 0m 10s) Loss: 0.0904(0.1134) \n",
      "EVAL: [40/62] Elapsed 0m 9s (remain 0m 4s) Loss: 0.1034(0.1150) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.1026  avg_val_loss: 0.1151  time: 132s\n",
      "Epoch 3 - Score: 0.4815  Scores: [0.5139244879788463, 0.46487266872059346, 0.44085451688797833, 0.4821287288160072, 0.49506517299121566, 0.4922677365430486]\n",
      "Epoch 3 - Save Best Score: 0.4815 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 14s (remain 0m 0s) Loss: 0.0925(0.1150) \n",
      "EVAL: [61/62] Elapsed 0m 14s (remain 0m 0s) Loss: 0.1801(0.1151) \n",
      "Epoch: [4][0/366] Elapsed 0m 0s (remain 4m 48s) Loss: 0.1298(0.1298) Grad: 117366.3594  LR: 0.00000294  \n",
      "Epoch: [4][20/366] Elapsed 0m 6s (remain 1m 53s) Loss: 0.0843(0.0862) Grad: 130919.3281  LR: 0.00000264  \n",
      "Epoch: [4][40/366] Elapsed 0m 13s (remain 1m 46s) Loss: 0.0798(0.0884) Grad: 59822.2188  LR: 0.00000236  \n",
      "Epoch: [4][60/366] Elapsed 0m 20s (remain 1m 41s) Loss: 0.1300(0.0919) Grad: 123847.5625  LR: 0.00000209  \n",
      "Epoch: [4][80/366] Elapsed 0m 26s (remain 1m 33s) Loss: 0.0725(0.0942) Grad: 99769.5312  LR: 0.00000183  \n",
      "Epoch: [4][100/366] Elapsed 0m 33s (remain 1m 27s) Loss: 0.1316(0.0958) Grad: 107257.8906  LR: 0.00000159  \n",
      "Epoch: [4][120/366] Elapsed 0m 39s (remain 1m 20s) Loss: 0.0755(0.0969) Grad: 69643.3281  LR: 0.00000137  \n",
      "Epoch: [4][140/366] Elapsed 0m 46s (remain 1m 13s) Loss: 0.0752(0.0973) Grad: 120740.1406  LR: 0.00000116  \n",
      "Epoch: [4][160/366] Elapsed 0m 52s (remain 1m 6s) Loss: 0.0834(0.0970) Grad: 70409.7188  LR: 0.00000097  \n",
      "Epoch: [4][180/366] Elapsed 0m 58s (remain 1m 0s) Loss: 0.0876(0.0962) Grad: 107844.2969  LR: 0.00000079  \n",
      "Epoch: [4][200/366] Elapsed 1m 5s (remain 0m 53s) Loss: 0.1063(0.0953) Grad: 101099.2969  LR: 0.00000063  \n",
      "Epoch: [4][220/366] Elapsed 1m 11s (remain 0m 47s) Loss: 0.0966(0.0946) Grad: 143611.4844  LR: 0.00000049  \n",
      "Epoch: [4][240/366] Elapsed 1m 17s (remain 0m 40s) Loss: 0.0642(0.0957) Grad: 48680.9844  LR: 0.00000037  \n",
      "Epoch: [4][260/366] Elapsed 1m 24s (remain 0m 33s) Loss: 0.0998(0.0950) Grad: 125542.0156  LR: 0.00000026  \n",
      "Epoch: [4][280/366] Elapsed 1m 31s (remain 0m 27s) Loss: 0.0887(0.0955) Grad: 72188.0781  LR: 0.00000017  \n",
      "Epoch: [4][300/366] Elapsed 1m 37s (remain 0m 21s) Loss: 0.0768(0.0953) Grad: 79606.6484  LR: 0.00000010  \n",
      "Epoch: [4][320/366] Elapsed 1m 44s (remain 0m 14s) Loss: 0.0728(0.0952) Grad: 194998.6406  LR: 0.00000005  \n",
      "Epoch: [4][340/366] Elapsed 1m 50s (remain 0m 8s) Loss: 0.1204(0.0952) Grad: 193668.4688  LR: 0.00000002  \n",
      "Epoch: [4][360/366] Elapsed 1m 56s (remain 0m 1s) Loss: 0.0950(0.0953) Grad: 149049.3125  LR: 0.00000000  \n",
      "Epoch: [4][365/366] Elapsed 1m 57s (remain 0m 0s) Loss: 0.0965(0.0951) Grad: 164624.6562  LR: 0.00000000  \n",
      "EVAL: [0/62] Elapsed 0m 0s (remain 0m 50s) Loss: 0.1244(0.1244) \n",
      "EVAL: [20/62] Elapsed 0m 5s (remain 0m 10s) Loss: 0.0850(0.1124) \n",
      "EVAL: [40/62] Elapsed 0m 9s (remain 0m 4s) Loss: 0.0991(0.1137) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0951  avg_val_loss: 0.1141  time: 132s\n",
      "Epoch 4 - Score: 0.4794  Scores: [0.5099867082801764, 0.46497887708457236, 0.4410143723262784, 0.4810482805557443, 0.49137899522601247, 0.48817451616749014]\n",
      "Epoch 4 - Save Best Score: 0.4794 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 14s (remain 0m 0s) Loss: 0.0932(0.1140) \n",
      "EVAL: [61/62] Elapsed 0m 14s (remain 0m 0s) Loss: 0.1561(0.1141) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 2 result ==========\n",
      "Score: 0.4794  Scores: [0.5099867082801764, 0.46497887708457236, 0.4410143723262784, 0.4810482805557443, 0.49137899522601247, 0.48817451616749014]\n",
      "========== fold: 3 training ==========\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "LSGElectraConfig {\n",
      "  \"_name_or_path\": \"/root/autodl-tmp/lsg-electra-base/\",\n",
      "  \"adaptive\": true,\n",
      "  \"architectures\": [\n",
      "    \"LSGElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoModel\": \"modeling_lsg_electra.LSGElectraModel\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_lsg_electra.LSGElectraForCausalLM\",\n",
      "    \"AutoModelForMaskedLM\": \"modeling_lsg_electra.LSGElectraForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modeling_lsg_electra.LSGElectraForMultipleChoice\",\n",
      "    \"AutoModelForPreTraining\": \"modeling_lsg_electra.LSGElectraForPreTraining\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modeling_lsg_electra.LSGElectraForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modeling_lsg_electra.LSGElectraForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modeling_lsg_electra.LSGElectraForTokenClassification\"\n",
      "  },\n",
      "  \"base_model_prefix\": \"lsg\",\n",
      "  \"block_size\": 128,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"lsh_num_pre_rounds\": 1,\n",
      "  \"mask_first_token\": false,\n",
      "  \"max_position_embeddings\": 1536,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_global_tokens\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pool_with_global\": true,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sparse_block_size\": 128,\n",
      "  \"sparsity_factor\": 2,\n",
      "  \"sparsity_type\": \"norm\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Some weights of the model checkpoint at /root/autodl-tmp/lsg-electra-base/ were not used when initializing LSGElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing LSGElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LSGElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/366] Elapsed 0m 0s (remain 4m 41s) Loss: 2.4840(2.4840) Grad: inf  LR: 0.00002000  \n",
      "Epoch: [1][20/366] Elapsed 0m 6s (remain 1m 54s) Loss: 1.7957(2.1319) Grad: 222775.3438  LR: 0.00001999  \n",
      "Epoch: [1][40/366] Elapsed 0m 13s (remain 1m 48s) Loss: 0.5588(1.4103) Grad: 147241.7812  LR: 0.00001996  \n",
      "Epoch: [1][60/366] Elapsed 0m 20s (remain 1m 44s) Loss: 0.2347(1.0128) Grad: 75411.7188  LR: 0.00001991  \n",
      "Epoch: [1][80/366] Elapsed 0m 27s (remain 1m 35s) Loss: 0.1257(0.8170) Grad: 81024.6562  LR: 0.00001985  \n",
      "Epoch: [1][100/366] Elapsed 0m 33s (remain 1m 28s) Loss: 0.1856(0.6865) Grad: 107211.5703  LR: 0.00001977  \n",
      "Epoch: [1][120/366] Elapsed 0m 39s (remain 1m 20s) Loss: 0.0760(0.5950) Grad: 56333.1172  LR: 0.00001967  \n",
      "Epoch: [1][140/366] Elapsed 0m 46s (remain 1m 13s) Loss: 0.2551(0.5367) Grad: 249208.0312  LR: 0.00001955  \n",
      "Epoch: [1][160/366] Elapsed 0m 52s (remain 1m 6s) Loss: 0.1249(0.4888) Grad: 55104.0664  LR: 0.00001941  \n",
      "Epoch: [1][180/366] Elapsed 0m 58s (remain 1m 0s) Loss: 0.1392(0.4515) Grad: 60859.1875  LR: 0.00001926  \n",
      "Epoch: [1][200/366] Elapsed 1m 4s (remain 0m 53s) Loss: 0.2137(0.4244) Grad: 64293.0117  LR: 0.00001909  \n",
      "Epoch: [1][220/366] Elapsed 1m 10s (remain 0m 46s) Loss: 0.1258(0.3999) Grad: 67659.9453  LR: 0.00001890  \n",
      "Epoch: [1][240/366] Elapsed 1m 16s (remain 0m 39s) Loss: 0.1580(0.3801) Grad: 97364.9609  LR: 0.00001870  \n",
      "Epoch: [1][260/366] Elapsed 1m 23s (remain 0m 33s) Loss: 0.1095(0.3604) Grad: 51457.2070  LR: 0.00001848  \n",
      "Epoch: [1][280/366] Elapsed 1m 30s (remain 0m 27s) Loss: 0.2473(0.3442) Grad: 41347.4102  LR: 0.00001824  \n",
      "Epoch: [1][300/366] Elapsed 1m 37s (remain 0m 21s) Loss: 0.1111(0.3304) Grad: 36407.9570  LR: 0.00001799  \n",
      "Epoch: [1][320/366] Elapsed 1m 43s (remain 0m 14s) Loss: 0.0687(0.3174) Grad: 27951.0195  LR: 0.00001773  \n",
      "Epoch: [1][340/366] Elapsed 1m 49s (remain 0m 8s) Loss: 0.1284(0.3058) Grad: 87993.4453  LR: 0.00001745  \n",
      "Epoch: [1][360/366] Elapsed 1m 56s (remain 0m 1s) Loss: 0.0970(0.2958) Grad: 32873.6719  LR: 0.00001715  \n",
      "Epoch: [1][365/366] Elapsed 1m 58s (remain 0m 0s) Loss: 0.1315(0.2932) Grad: 34334.3047  LR: 0.00001708  \n",
      "EVAL: [0/62] Elapsed 0m 0s (remain 0m 51s) Loss: 0.1452(0.1452) \n",
      "EVAL: [20/62] Elapsed 0m 5s (remain 0m 10s) Loss: 0.0987(0.1201) \n",
      "EVAL: [40/62] Elapsed 0m 9s (remain 0m 5s) Loss: 0.1149(0.1237) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.2932  avg_val_loss: 0.1220  time: 133s\n",
      "Epoch 1 - Score: 0.4964  Scores: [0.526141610051238, 0.4681249307146075, 0.4816124163615339, 0.478038826008438, 0.5393247059276969, 0.48510072958342654]\n",
      "Epoch 1 - Save Best Score: 0.4964 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 14s (remain 0m 0s) Loss: 0.1046(0.1221) \n",
      "EVAL: [61/62] Elapsed 0m 14s (remain 0m 0s) Loss: 0.0842(0.1220) \n",
      "Epoch: [2][0/366] Elapsed 0m 0s (remain 4m 22s) Loss: 0.1221(0.1221) Grad: 151452.2344  LR: 0.00001706  \n",
      "Epoch: [2][20/366] Elapsed 0m 7s (remain 2m 3s) Loss: 0.1118(0.1164) Grad: 105299.7891  LR: 0.00001675  \n",
      "Epoch: [2][40/366] Elapsed 0m 14s (remain 1m 52s) Loss: 0.1221(0.1165) Grad: 98966.9609  LR: 0.00001643  \n",
      "Epoch: [2][60/366] Elapsed 0m 20s (remain 1m 42s) Loss: 0.1247(0.1149) Grad: 83631.2656  LR: 0.00001610  \n",
      "Epoch: [2][80/366] Elapsed 0m 26s (remain 1m 32s) Loss: 0.1108(0.1119) Grad: 121162.8594  LR: 0.00001575  \n",
      "Epoch: [2][100/366] Elapsed 0m 32s (remain 1m 25s) Loss: 0.1321(0.1109) Grad: 79939.7266  LR: 0.00001540  \n",
      "Epoch: [2][120/366] Elapsed 0m 38s (remain 1m 18s) Loss: 0.1807(0.1134) Grad: 281357.5000  LR: 0.00001503  \n",
      "Epoch: [2][140/366] Elapsed 0m 45s (remain 1m 12s) Loss: 0.1132(0.1134) Grad: 219319.6406  LR: 0.00001466  \n",
      "Epoch: [2][160/366] Elapsed 0m 51s (remain 1m 5s) Loss: 0.0398(0.1135) Grad: 68247.3203  LR: 0.00001427  \n",
      "Epoch: [2][180/366] Elapsed 0m 58s (remain 1m 0s) Loss: 0.1721(0.1161) Grad: 325341.5938  LR: 0.00001388  \n",
      "Epoch: [2][200/366] Elapsed 1m 4s (remain 0m 53s) Loss: 0.0558(0.1163) Grad: 65027.0234  LR: 0.00001348  \n",
      "Epoch: [2][220/366] Elapsed 1m 11s (remain 0m 46s) Loss: 0.0842(0.1159) Grad: 137511.3125  LR: 0.00001308  \n",
      "Epoch: [2][240/366] Elapsed 1m 17s (remain 0m 40s) Loss: 0.0688(0.1156) Grad: 64376.1992  LR: 0.00001267  \n",
      "Epoch: [2][260/366] Elapsed 1m 23s (remain 0m 33s) Loss: 0.1071(0.1154) Grad: 191795.1562  LR: 0.00001225  \n",
      "Epoch: [2][280/366] Elapsed 1m 30s (remain 0m 27s) Loss: 0.1398(0.1152) Grad: 138994.1250  LR: 0.00001183  \n",
      "Epoch: [2][300/366] Elapsed 1m 37s (remain 0m 21s) Loss: 0.1910(0.1161) Grad: 125000.4609  LR: 0.00001141  \n",
      "Epoch: [2][320/366] Elapsed 1m 43s (remain 0m 14s) Loss: 0.2178(0.1160) Grad: 213108.7031  LR: 0.00001098  \n",
      "Epoch: [2][340/366] Elapsed 1m 50s (remain 0m 8s) Loss: 0.1033(0.1155) Grad: 83023.9609  LR: 0.00001056  \n",
      "Epoch: [2][360/366] Elapsed 1m 56s (remain 0m 1s) Loss: 0.1406(0.1157) Grad: 187126.9375  LR: 0.00001013  \n",
      "Epoch: [2][365/366] Elapsed 1m 58s (remain 0m 0s) Loss: 0.0830(0.1158) Grad: 115742.0938  LR: 0.00001002  \n",
      "EVAL: [0/62] Elapsed 0m 0s (remain 0m 50s) Loss: 0.1105(0.1105) \n",
      "EVAL: [20/62] Elapsed 0m 5s (remain 0m 10s) Loss: 0.0865(0.1082) \n",
      "EVAL: [40/62] Elapsed 0m 9s (remain 0m 5s) Loss: 0.1053(0.1133) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1158  avg_val_loss: 0.1133  time: 133s\n",
      "Epoch 2 - Score: 0.4771  Scores: [0.5075631227246158, 0.4567200843366543, 0.4383141646699799, 0.4620412870332408, 0.5300047739445778, 0.46818674871812177]\n",
      "Epoch 2 - Save Best Score: 0.4771 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 14s (remain 0m 0s) Loss: 0.0995(0.1134) \n",
      "EVAL: [61/62] Elapsed 0m 14s (remain 0m 0s) Loss: 0.0533(0.1133) \n",
      "Epoch: [3][0/366] Elapsed 0m 0s (remain 4m 39s) Loss: 0.0797(0.0797) Grad: 63733.4414  LR: 0.00001000  \n",
      "Epoch: [3][20/366] Elapsed 0m 7s (remain 1m 59s) Loss: 0.0904(0.0958) Grad: 112811.1641  LR: 0.00000957  \n",
      "Epoch: [3][40/366] Elapsed 0m 13s (remain 1m 47s) Loss: 0.1096(0.0979) Grad: 118858.2656  LR: 0.00000914  \n",
      "Epoch: [3][60/366] Elapsed 0m 19s (remain 1m 39s) Loss: 0.1443(0.1016) Grad: 173066.6562  LR: 0.00000872  \n",
      "Epoch: [3][80/366] Elapsed 0m 26s (remain 1m 33s) Loss: 0.0580(0.1039) Grad: 88477.8828  LR: 0.00000829  \n",
      "Epoch: [3][100/366] Elapsed 0m 32s (remain 1m 26s) Loss: 0.0941(0.1049) Grad: 80804.2188  LR: 0.00000787  \n",
      "Epoch: [3][120/366] Elapsed 0m 39s (remain 1m 19s) Loss: 0.1000(0.1045) Grad: 107389.8438  LR: 0.00000746  \n",
      "Epoch: [3][140/366] Elapsed 0m 46s (remain 1m 13s) Loss: 0.1091(0.1042) Grad: 149540.7656  LR: 0.00000704  \n",
      "Epoch: [3][160/366] Elapsed 0m 52s (remain 1m 7s) Loss: 0.1087(0.1032) Grad: 116936.9219  LR: 0.00000664  \n",
      "Epoch: [3][180/366] Elapsed 0m 58s (remain 1m 0s) Loss: 0.1249(0.1039) Grad: 97227.7812  LR: 0.00000624  \n",
      "Epoch: [3][200/366] Elapsed 1m 5s (remain 0m 53s) Loss: 0.1227(0.1042) Grad: 86138.3438  LR: 0.00000584  \n",
      "Epoch: [3][220/366] Elapsed 1m 11s (remain 0m 46s) Loss: 0.0514(0.1037) Grad: 66880.1875  LR: 0.00000546  \n",
      "Epoch: [3][240/366] Elapsed 1m 17s (remain 0m 40s) Loss: 0.0783(0.1027) Grad: 63358.9844  LR: 0.00000508  \n",
      "Epoch: [3][260/366] Elapsed 1m 24s (remain 0m 33s) Loss: 0.1334(0.1030) Grad: 82446.4375  LR: 0.00000471  \n",
      "Epoch: [3][280/366] Elapsed 1m 30s (remain 0m 27s) Loss: 0.0673(0.1024) Grad: 167738.1875  LR: 0.00000435  \n",
      "Epoch: [3][300/366] Elapsed 1m 36s (remain 0m 20s) Loss: 0.0634(0.1023) Grad: 57510.6914  LR: 0.00000400  \n",
      "Epoch: [3][320/366] Elapsed 1m 43s (remain 0m 14s) Loss: 0.0676(0.1021) Grad: 107358.7734  LR: 0.00000367  \n",
      "Epoch: [3][340/366] Elapsed 1m 49s (remain 0m 8s) Loss: 0.1863(0.1023) Grad: 127220.0234  LR: 0.00000334  \n",
      "Epoch: [3][360/366] Elapsed 1m 56s (remain 0m 1s) Loss: 0.1347(0.1028) Grad: 288088.4062  LR: 0.00000303  \n",
      "Epoch: [3][365/366] Elapsed 1m 58s (remain 0m 0s) Loss: 0.1234(0.1031) Grad: 121364.4531  LR: 0.00000295  \n",
      "EVAL: [0/62] Elapsed 0m 0s (remain 0m 48s) Loss: 0.1104(0.1104) \n",
      "EVAL: [20/62] Elapsed 0m 5s (remain 0m 10s) Loss: 0.0815(0.1042) \n",
      "EVAL: [40/62] Elapsed 0m 9s (remain 0m 5s) Loss: 0.1141(0.1089) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.1031  avg_val_loss: 0.1084  time: 133s\n",
      "Epoch 3 - Score: 0.4668  Scores: [0.5034888864469952, 0.45447456478792947, 0.4325065592318034, 0.4550091262223107, 0.49270537132782116, 0.4626257003280509]\n",
      "Epoch 3 - Save Best Score: 0.4668 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 14s (remain 0m 0s) Loss: 0.0937(0.1086) \n",
      "EVAL: [61/62] Elapsed 0m 14s (remain 0m 0s) Loss: 0.0442(0.1084) \n",
      "Epoch: [4][0/366] Elapsed 0m 0s (remain 5m 32s) Loss: 0.0714(0.0714) Grad: 80821.5156  LR: 0.00000294  \n",
      "Epoch: [4][20/366] Elapsed 0m 7s (remain 2m 9s) Loss: 0.1418(0.1026) Grad: 142979.8750  LR: 0.00000264  \n",
      "Epoch: [4][40/366] Elapsed 0m 14s (remain 1m 51s) Loss: 0.2050(0.1047) Grad: 170244.7656  LR: 0.00000236  \n",
      "Epoch: [4][60/366] Elapsed 0m 20s (remain 1m 40s) Loss: 0.0750(0.1015) Grad: 99660.9297  LR: 0.00000209  \n",
      "Epoch: [4][80/366] Elapsed 0m 26s (remain 1m 33s) Loss: 0.1386(0.1010) Grad: 110946.5000  LR: 0.00000183  \n",
      "Epoch: [4][100/366] Elapsed 0m 33s (remain 1m 27s) Loss: 0.0600(0.1001) Grad: 69135.7422  LR: 0.00000159  \n",
      "Epoch: [4][120/366] Elapsed 0m 39s (remain 1m 19s) Loss: 0.0811(0.1007) Grad: 71444.8047  LR: 0.00000137  \n",
      "Epoch: [4][140/366] Elapsed 0m 46s (remain 1m 13s) Loss: 0.0554(0.0991) Grad: 88400.7188  LR: 0.00000116  \n",
      "Epoch: [4][160/366] Elapsed 0m 52s (remain 1m 6s) Loss: 0.1473(0.0977) Grad: 136902.3125  LR: 0.00000097  \n",
      "Epoch: [4][180/366] Elapsed 0m 58s (remain 1m 0s) Loss: 0.0699(0.0981) Grad: 107294.5078  LR: 0.00000079  \n",
      "Epoch: [4][200/366] Elapsed 1m 5s (remain 0m 53s) Loss: 0.1161(0.0976) Grad: 102564.1875  LR: 0.00000063  \n",
      "Epoch: [4][220/366] Elapsed 1m 11s (remain 0m 46s) Loss: 0.0942(0.0971) Grad: 80167.0078  LR: 0.00000049  \n",
      "Epoch: [4][240/366] Elapsed 1m 17s (remain 0m 40s) Loss: 0.1242(0.0971) Grad: 135726.7812  LR: 0.00000037  \n",
      "Epoch: [4][260/366] Elapsed 1m 24s (remain 0m 33s) Loss: 0.1031(0.0983) Grad: 77936.9922  LR: 0.00000026  \n",
      "Epoch: [4][280/366] Elapsed 1m 30s (remain 0m 27s) Loss: 0.0910(0.0981) Grad: 145294.4844  LR: 0.00000017  \n",
      "Epoch: [4][300/366] Elapsed 1m 36s (remain 0m 20s) Loss: 0.0837(0.0974) Grad: 90863.8828  LR: 0.00000010  \n",
      "Epoch: [4][320/366] Elapsed 1m 43s (remain 0m 14s) Loss: 0.1251(0.0972) Grad: 91982.9453  LR: 0.00000005  \n",
      "Epoch: [4][340/366] Elapsed 1m 49s (remain 0m 8s) Loss: 0.0614(0.0968) Grad: 84167.7656  LR: 0.00000002  \n",
      "Epoch: [4][360/366] Elapsed 1m 56s (remain 0m 1s) Loss: 0.0705(0.0962) Grad: 143689.8438  LR: 0.00000000  \n",
      "Epoch: [4][365/366] Elapsed 1m 58s (remain 0m 0s) Loss: 0.1193(0.0962) Grad: 118457.2109  LR: 0.00000000  \n",
      "EVAL: [0/62] Elapsed 0m 0s (remain 0m 49s) Loss: 0.1078(0.1078) \n",
      "EVAL: [20/62] Elapsed 0m 5s (remain 0m 10s) Loss: 0.0820(0.1041) \n",
      "EVAL: [40/62] Elapsed 0m 9s (remain 0m 5s) Loss: 0.1136(0.1085) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0962  avg_val_loss: 0.1082  time: 133s\n",
      "Epoch 4 - Score: 0.4663  Scores: [0.5027185538033578, 0.4533606108598723, 0.4309384753262597, 0.4550886891511836, 0.4928231639087349, 0.4629594929363798]\n",
      "Epoch 4 - Save Best Score: 0.4663 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 14s (remain 0m 0s) Loss: 0.0912(0.1083) \n",
      "EVAL: [61/62] Elapsed 0m 14s (remain 0m 0s) Loss: 0.0479(0.1082) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 3 result ==========\n",
      "Score: 0.4663  Scores: [0.5027185538033578, 0.4533606108598723, 0.4309384753262597, 0.4550886891511836, 0.4928231639087349, 0.4629594929363798]\n",
      "========== CV ==========\n",
      "Score: 0.4724  Scores: [0.5061653231580762, 0.4624927400134135, 0.43269011127595514, 0.46738706941242064, 0.49494898435707757, 0.4708564904202183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finised\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    def get_result(oof_df):\n",
    "        labels = oof_df[CFG.target_cols].values\n",
    "        preds = oof_df[[f\"pred_{c}\" for c in CFG.target_cols]].values\n",
    "        score, scores = get_score(labels, preds)\n",
    "        LOGGER.info(f'Score: {score:<.4f}  Scores: {scores}')\n",
    "    \n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                _oof_df = train_loop(train, fold)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "                get_result(_oof_df)\n",
    "        oof_df = oof_df.reset_index(drop=True)\n",
    "        LOGGER.info(f\"========== CV ==========\")\n",
    "        get_result(oof_df)\n",
    "        oof_df.to_pickle(OUTPUT_DIR+'oof_df.pkl')\n",
    "        \n",
    "    print('finised')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251828a2-f86b-44e3-ae54-561b93f18715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be9cd73-62ae-4435-8a85-332d29a1de4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
