{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b8b30af-743a-45b8-9710-fc3567665c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.21.2\n",
    "# !pip install tokenizers==0.12.1\n",
    "\n",
    "# !pip install -q joblib scikit-learn scipy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09ac4f52-194d-41e4-82d1-fda0adb599c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://repo.huaweicloud.com/repository/pypi/simple\n",
      "Requirement already satisfied: iterative-stratification==0.1.7 in /root/miniconda3/lib/python3.8/site-packages (0.1.7)\n",
      "Requirement already satisfied: scipy in /root/miniconda3/lib/python3.8/site-packages (from iterative-stratification==0.1.7) (1.9.3)\n",
      "Requirement already satisfied: scikit-learn in /root/miniconda3/lib/python3.8/site-packages (from iterative-stratification==0.1.7) (1.1.2)\n",
      "Requirement already satisfied: numpy in /root/miniconda3/lib/python3.8/site-packages (from iterative-stratification==0.1.7) (1.22.4)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /root/miniconda3/lib/python3.8/site-packages (from scikit-learn->iterative-stratification==0.1.7) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /root/miniconda3/lib/python3.8/site-packages (from scikit-learn->iterative-stratification==0.1.7) (3.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers.__version__: 0.12.1\n",
      "transformers.__version__: 4.21.2\n",
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "\n",
    "import math\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "os.system('pip install iterative-stratification==0.1.7')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "import tokenizers\n",
    "import transformers\n",
    "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc1574b-51ec-4fd1-aed7-c56f30354740",
   "metadata": {},
   "source": [
    "# CFG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bec36cda-23f1-4e77-b26c-c8e46b01844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_P = '/root/autodl-tmp/fb3/inputs/'\n",
    "OUTPUT_DIR = '/root/autodl-tmp/fb3/output/trained_tiny_model/lsg-roberta-large/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5f643ff-884a-4d3d-a464-30cff8ca30af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    wandb=False\n",
    "    # competition='FB3'\n",
    "    # _wandb_kernel='nakama'\n",
    "    \n",
    "    path=f\"{DATA_P}common-nlp-tokenizer/model_tokenizer/tiny/lsg-roberta-large/\"\n",
    "    model=path\n",
    "    config_path=model+'config.pth'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    cfg_save_output = OUTPUT_DIR\n",
    "\n",
    "    debug=False\n",
    "    train=True\n",
    "    trust_remote_code=True\n",
    "    \n",
    "    apex=True\n",
    "    print_freq=20\n",
    "    num_workers=4\n",
    "    gradient_checkpointing=True\n",
    "    scheduler='cosine' # ['linear', 'cosine']\n",
    "    batch_scheduler=True\n",
    "    num_cycles=0.5\n",
    "    num_warmup_steps=0\n",
    "    epochs=4\n",
    "    encoder_lr=2e-5\n",
    "    decoder_lr=2e-5\n",
    "    min_lr=1e-6\n",
    "    eps=1e-6\n",
    "    betas=(0.9, 0.999)\n",
    "    batch_size=8\n",
    "    weight_decay=0.01\n",
    "    gradient_accumulation_steps=1\n",
    "    max_grad_norm=1000\n",
    "    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "    seed=42\n",
    "    n_fold=4\n",
    "    trn_fold=[0, 1, 2, 3]\n",
    "    max_len=1536\n",
    "    \n",
    "if CFG.debug:\n",
    "    CFG.epochs = 2\n",
    "    CFG.trn_fold = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd54f4c9-0e59-4750-8fd3-fa19763cf32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ====================================================\n",
    "# # tokenizer\n",
    "# # ====================================================\n",
    "# tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
    "# # tokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\n",
    "# CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1079d60-08d8-44e9-be5a-d59a1a1e388d",
   "metadata": {},
   "source": [
    "# Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5883b61b-c09f-416d-81f0-f2292d0fc887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "\n",
    "def MCRMSE(y_trues, y_preds):\n",
    "    scores = []\n",
    "    idxes = y_trues.shape[1]\n",
    "    for i in range(idxes):\n",
    "        y_true = y_trues[:,i]\n",
    "        y_pred = y_preds[:,i]\n",
    "        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n",
    "        scores.append(score)\n",
    "    mcrmse_score = np.mean(scores)\n",
    "    return mcrmse_score, scores\n",
    "\n",
    "\n",
    "def get_score(y_trues, y_preds):\n",
    "    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n",
    "    return mcrmse_score, scores\n",
    "\n",
    "\n",
    "def get_logger(filename=OUTPUT_DIR+'train'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbd48c0-7b8c-4361-8a0a-d3b1930a267c",
   "metadata": {},
   "source": [
    "##  Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b5c4856-a18e-4552-9dff-1f31e9a4ecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Data Loading\n",
    "# ====================================================\n",
    "train = pd.read_csv(f'{DATA_P}feedback-prize-english-language-learning/train.csv')\n",
    "test = pd.read_csv(f'{DATA_P}feedback-prize-english-language-learning/test.csv')\n",
    "submission = pd.read_csv(f'{DATA_P}feedback-prize-english-language-learning/sample_submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4318c6c2-679b-4689-ae5e-459463304d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    978\n",
       "1    977\n",
       "2    978\n",
       "3    978\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# CV split\n",
    "# ====================================================\n",
    "Fold = MultilabelStratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train[CFG.target_cols])):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "display(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "765402fa-a541-45ee-88ca-305bee3113a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    display(train.groupby('fold').size())\n",
    "    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n",
    "    display(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f719884f-ff13-4742-b9bf-e346b29bbe23",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e231bc7-d317-4aa1-b0e6-ffd670974143",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_len: 1536\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Define max_len\n",
    "# ====================================================\n",
    "# lengths = []\n",
    "# tk0 = tqdm(train['full_text'].fillna(\"\").values, total=len(train))\n",
    "# for text in tk0:\n",
    "#     length = len(CFG.tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "#     lengths.append(length)\n",
    "# CFG.max_len = max(lengths) + 3 # cls & sep & sep\n",
    "LOGGER.info(f\"max_len: {CFG.max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dff7a599-7bf6-4e9e-bc08-c6eb2742d298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "def prepare_input(cfg, text):\n",
    "    inputs = cfg.tokenizer.encode_plus(\n",
    "        text, \n",
    "        return_tensors=None, \n",
    "        add_special_tokens=True, \n",
    "        max_length=CFG.max_len,\n",
    "        pad_to_max_length=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.texts = df['full_text'].values\n",
    "        self.labels = df[cfg.target_cols].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.cfg, self.texts[item])\n",
    "        label = torch.tensor(self.labels[item], dtype=torch.float)\n",
    "        return inputs, label\n",
    "    \n",
    "\n",
    "def collate(inputs):\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b00ae5-9cb9-49f7-8831-4f5b8991965c",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d72c2e5e-aeca-4fa8-bd47-051b40d7b328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "    \n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True, trust_remote_code=True)\n",
    "            self.config.hidden_dropout = 0.\n",
    "            self.config.hidden_dropout_prob = 0.\n",
    "            self.config.attention_dropout = 0.\n",
    "            self.config.attention_probs_dropout_prob = 0.\n",
    "            LOGGER.info(self.config)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(cfg.model, config=self.config, trust_remote_code=True)\n",
    "        else:\n",
    "            self.model = AutoModel(self.config)\n",
    "        if self.cfg.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        self.pool = MeanPooling()\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 6)\n",
    "        self._init_weights(self.fc)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        feature = self.pool(last_hidden_states, inputs['attention_mask'])\n",
    "        return feature\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(feature)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "# ====================================================\n",
    "# Loss\n",
    "# ====================================================\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, reduction='mean', eps=1e-9):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss(reduction='none')\n",
    "        self.reduction = reduction\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = torch.sqrt(self.mse(y_pred, y_true) + self.eps)\n",
    "        if self.reduction == 'none':\n",
    "            loss = loss\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "# ====================================================\n",
    "# Helper functions\n",
    "# ====================================================\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            y_preds = model(inputs)\n",
    "            loss = criterion(y_preds, labels)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            if CFG.batch_scheduler:\n",
    "                scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch+1, step, len(train_loader), \n",
    "                          remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "        if CFG.wandb:\n",
    "            wandb.log({f\"[fold{fold}] loss\": losses.val,\n",
    "                       f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs, labels) in enumerate(valid_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "            loss = criterion(y_preds, labels)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbf1214-951b-472a-b2e6-33581fbb4be3",
   "metadata": {},
   "source": [
    "# train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dfe6437-8227-4eef-9789-182c9ae029f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# train loop\n",
    "# ====================================================\n",
    "def train_loop(folds, fold):\n",
    "    \n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n",
    "    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n",
    "    valid_labels = valid_folds[CFG.target_cols].values\n",
    "    \n",
    "    train_dataset = TrainDataset(CFG, train_folds)\n",
    "    valid_dataset = TrainDataset(CFG, valid_folds)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    model = CustomModel(CFG, config_path=None, pretrained=True)\n",
    "    torch.save(model.config, OUTPUT_DIR+'config.pth')\n",
    "    model.to(device)\n",
    "    \n",
    "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "             'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "\n",
    "    optimizer_parameters = get_optimizer_params(model,\n",
    "                                                encoder_lr=CFG.encoder_lr, \n",
    "                                                decoder_lr=CFG.decoder_lr,\n",
    "                                                weight_decay=CFG.weight_decay)\n",
    "    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n",
    "    \n",
    "    # ====================================================\n",
    "    # scheduler\n",
    "    # ====================================================\n",
    "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        if cfg.scheduler == 'linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "            )\n",
    "        elif cfg.scheduler == 'cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "            )\n",
    "        return scheduler\n",
    "    \n",
    "    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    criterion = nn.SmoothL1Loss(reduction='mean') # RMSELoss(reduction=\"mean\")\n",
    "    \n",
    "    best_score = np.inf\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n",
    "        \n",
    "        # scoring\n",
    "        score, scores = get_score(valid_labels, predictions)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}  Scores: {scores}')\n",
    "        if CFG.wandb:\n",
    "            wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n",
    "                       f\"[fold{fold}] avg_train_loss\": avg_loss, \n",
    "                       f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n",
    "                       f\"[fold{fold}] score\": score})\n",
    "        \n",
    "        if best_score > score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                        f\"{CFG.cfg_save_output}_fold{fold}_best.pth\")\n",
    "                        # OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n",
    "\n",
    "            cfg_save_output = f\"{OUTPUT_DIR}trained_tiny_model/lsg-electra-base/\"\n",
    "            \n",
    "            \n",
    "    # predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", \n",
    "    #                          map_location=torch.device('cpu'))['predictions']\n",
    "    predictions = torch.load(f\"{CFG.cfg_save_output}_fold{fold}_best.pth\", \n",
    "                             map_location=torch.device('cpu'))['predictions']\n",
    "    valid_folds[[f\"pred_{c}\" for c in CFG.target_cols]] = predictions\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return valid_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dde3fb90-871c-4528-94ef-3437b8de1bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "LSGRobertaConfig {\n",
      "  \"_name_or_path\": \"/root/autodl-tmp/fb3/inputs/common-nlp-tokenizer/model_tokenizer/tiny/lsg-roberta-large/\",\n",
      "  \"adaptive\": true,\n",
      "  \"architectures\": [\n",
      "    \"LSGRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoModel\": \"modeling_lsg_roberta.LSGRobertaModel\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_lsg_roberta.LSGRobertaForCausalLM\",\n",
      "    \"AutoModelForMaskedLM\": \"modeling_lsg_roberta.LSGRobertaForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modeling_lsg_roberta.LSGRobertaForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modeling_lsg_roberta.LSGRobertaForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modeling_lsg_roberta.LSGRobertaForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modeling_lsg_roberta.LSGRobertaForTokenClassification\"\n",
      "  },\n",
      "  \"base_model_prefix\": \"lsg\",\n",
      "  \"block_size\": 128,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"lsh_num_pre_rounds\": 1,\n",
      "  \"mask_first_token\": false,\n",
      "  \"max_position_embeddings\": 1538,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_global_tokens\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pool_with_global\": true,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sparse_block_size\": 128,\n",
      "  \"sparsity_factor\": 2,\n",
      "  \"sparsity_type\": \"norm\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Some weights of the model checkpoint at /root/autodl-tmp/fb3/inputs/common-nlp-tokenizer/model_tokenizer/tiny/lsg-roberta-large/ were not used when initializing LSGRobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing LSGRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LSGRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LSGRobertaModel were not initialized from the model checkpoint at /root/autodl-tmp/fb3/inputs/common-nlp-tokenizer/model_tokenizer/tiny/lsg-roberta-large/ and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/366] Elapsed 0m 1s (remain 10m 30s) Loss: 2.6484(2.6484) Grad: inf  LR: 0.00002000  \n",
      "Epoch: [1][20/366] Elapsed 0m 22s (remain 6m 17s) Loss: 0.2559(1.0182) Grad: 72839.5312  LR: 0.00001999  \n",
      "Epoch: [1][40/366] Elapsed 0m 42s (remain 5m 38s) Loss: 0.1860(0.6095) Grad: 137043.0156  LR: 0.00001996  \n",
      "Epoch: [1][60/366] Elapsed 1m 2s (remain 5m 12s) Loss: 0.2269(0.4655) Grad: 102770.7031  LR: 0.00001991  \n",
      "Epoch: [1][80/366] Elapsed 1m 22s (remain 4m 51s) Loss: 0.1589(0.3859) Grad: 58324.6406  LR: 0.00001985  \n",
      "Epoch: [1][100/366] Elapsed 1m 41s (remain 4m 26s) Loss: 0.1973(0.3394) Grad: 112786.6719  LR: 0.00001977  \n",
      "Epoch: [1][120/366] Elapsed 2m 0s (remain 4m 4s) Loss: 0.1717(0.3072) Grad: 51925.4648  LR: 0.00001967  \n",
      "Epoch: [1][140/366] Elapsed 2m 20s (remain 3m 44s) Loss: 0.1548(0.2837) Grad: 32417.6367  LR: 0.00001955  \n",
      "Epoch: [1][160/366] Elapsed 2m 42s (remain 3m 27s) Loss: 0.1442(0.2664) Grad: 41136.9414  LR: 0.00001941  \n",
      "Epoch: [1][180/366] Elapsed 3m 3s (remain 3m 7s) Loss: 0.1950(0.2519) Grad: 82293.3984  LR: 0.00001926  \n",
      "Epoch: [1][200/366] Elapsed 3m 27s (remain 2m 50s) Loss: 0.0955(0.2391) Grad: 31593.8848  LR: 0.00001909  \n",
      "Epoch: [1][220/366] Elapsed 3m 49s (remain 2m 30s) Loss: 0.1698(0.2310) Grad: 88461.7969  LR: 0.00001890  \n",
      "Epoch: [1][240/366] Elapsed 4m 8s (remain 2m 9s) Loss: 0.1040(0.2213) Grad: 29988.5703  LR: 0.00001870  \n",
      "Epoch: [1][260/366] Elapsed 4m 29s (remain 1m 48s) Loss: 0.1540(0.2129) Grad: 38288.3750  LR: 0.00001848  \n",
      "Epoch: [1][280/366] Elapsed 4m 49s (remain 1m 27s) Loss: 0.1486(0.2068) Grad: 27660.9082  LR: 0.00001824  \n",
      "Epoch: [1][300/366] Elapsed 5m 8s (remain 1m 6s) Loss: 0.1097(0.2011) Grad: 26947.2305  LR: 0.00001799  \n",
      "Epoch: [1][320/366] Elapsed 5m 28s (remain 0m 45s) Loss: 0.1176(0.1967) Grad: 45544.5859  LR: 0.00001773  \n",
      "Epoch: [1][340/366] Elapsed 5m 46s (remain 0m 25s) Loss: 0.1479(0.1920) Grad: 37194.4375  LR: 0.00001745  \n",
      "Epoch: [1][360/366] Elapsed 6m 7s (remain 0m 5s) Loss: 0.0671(0.1896) Grad: 26242.3867  LR: 0.00001715  \n",
      "Epoch: [1][365/366] Elapsed 6m 11s (remain 0m 0s) Loss: 0.1289(0.1887) Grad: 54232.3242  LR: 0.00001708  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 40s) Loss: 0.0765(0.0765) \n",
      "EVAL: [20/62] Elapsed 0m 28s (remain 0m 56s) Loss: 0.0724(0.1050) \n",
      "EVAL: [40/62] Elapsed 0m 54s (remain 0m 28s) Loss: 0.0889(0.1049) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1887  avg_val_loss: 0.1071  time: 452s\n",
      "Epoch 1 - Score: 0.4632  Scores: [0.5115188405499517, 0.4557977532004715, 0.4232776800288024, 0.462409202057009, 0.48106842611800643, 0.4453430347602914]\n",
      "Epoch 1 - Save Best Score: 0.4632 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 1m 19s (remain 0m 1s) Loss: 0.1008(0.1071) \n",
      "EVAL: [61/62] Elapsed 1m 20s (remain 0m 0s) Loss: 0.1022(0.1071) \n",
      "Epoch: [2][0/366] Elapsed 0m 1s (remain 9m 16s) Loss: 0.1085(0.1085) Grad: 323820.5938  LR: 0.00001706  \n",
      "Epoch: [2][20/366] Elapsed 0m 21s (remain 5m 56s) Loss: 0.1970(0.1726) Grad: 472716.5938  LR: 0.00001675  \n",
      "Epoch: [2][40/366] Elapsed 0m 42s (remain 5m 37s) Loss: 0.0865(0.1433) Grad: 33924.3789  LR: 0.00001643  \n",
      "Epoch: [2][60/366] Elapsed 1m 3s (remain 5m 15s) Loss: 0.1825(0.1299) Grad: 108181.2891  LR: 0.00001610  \n",
      "Epoch: [2][80/366] Elapsed 1m 23s (remain 4m 55s) Loss: 0.1406(0.1217) Grad: 134916.7969  LR: 0.00001575  \n",
      "Epoch: [2][100/366] Elapsed 1m 44s (remain 4m 34s) Loss: 0.1316(0.1183) Grad: 99629.2266  LR: 0.00001540  \n",
      "Epoch: [2][120/366] Elapsed 2m 6s (remain 4m 16s) Loss: 0.1149(0.1141) Grad: 69112.9531  LR: 0.00001503  \n",
      "Epoch: [2][140/366] Elapsed 2m 27s (remain 3m 55s) Loss: 0.0729(0.1106) Grad: 61378.2227  LR: 0.00001466  \n",
      "Epoch: [2][160/366] Elapsed 2m 48s (remain 3m 34s) Loss: 0.0857(0.1084) Grad: 66233.6875  LR: 0.00001427  \n",
      "Epoch: [2][180/366] Elapsed 3m 8s (remain 3m 12s) Loss: 0.0864(0.1069) Grad: 51523.2188  LR: 0.00001388  \n",
      "Epoch: [2][200/366] Elapsed 3m 27s (remain 2m 50s) Loss: 0.0642(0.1052) Grad: 63434.9883  LR: 0.00001348  \n",
      "Epoch: [2][220/366] Elapsed 3m 47s (remain 2m 28s) Loss: 0.1212(0.1053) Grad: 40122.9727  LR: 0.00001308  \n",
      "Epoch: [2][240/366] Elapsed 4m 8s (remain 2m 9s) Loss: 0.1031(0.1042) Grad: 52120.3711  LR: 0.00001267  \n",
      "Epoch: [2][260/366] Elapsed 4m 27s (remain 1m 47s) Loss: 0.1264(0.1046) Grad: 106245.6172  LR: 0.00001225  \n",
      "Epoch: [2][280/366] Elapsed 4m 47s (remain 1m 26s) Loss: 0.1058(0.1039) Grad: 181204.9375  LR: 0.00001183  \n",
      "Epoch: [2][300/366] Elapsed 5m 6s (remain 1m 6s) Loss: 0.1186(0.1033) Grad: 46965.4375  LR: 0.00001141  \n",
      "Epoch: [2][320/366] Elapsed 5m 28s (remain 0m 46s) Loss: 0.0712(0.1031) Grad: 67842.4922  LR: 0.00001098  \n",
      "Epoch: [2][340/366] Elapsed 5m 46s (remain 0m 25s) Loss: 0.1642(0.1028) Grad: 132050.1250  LR: 0.00001056  \n",
      "Epoch: [2][360/366] Elapsed 6m 7s (remain 0m 5s) Loss: 0.0588(0.1021) Grad: 28823.9883  LR: 0.00001013  \n",
      "Epoch: [2][365/366] Elapsed 6m 12s (remain 0m 0s) Loss: 0.0774(0.1020) Grad: 48306.1211  LR: 0.00001002  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 41s) Loss: 0.0752(0.0752) \n",
      "EVAL: [20/62] Elapsed 0m 28s (remain 0m 56s) Loss: 0.0889(0.1156) \n",
      "EVAL: [40/62] Elapsed 0m 54s (remain 0m 28s) Loss: 0.0946(0.1142) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1020  avg_val_loss: 0.1143  time: 453s\n",
      "Epoch 2 - Score: 0.4789  Scores: [0.5152511361385136, 0.4746899850337709, 0.41382723528255944, 0.46422510384075977, 0.5045997067908694, 0.5009600319427913]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 1m 19s (remain 0m 1s) Loss: 0.1105(0.1145) \n",
      "EVAL: [61/62] Elapsed 1m 20s (remain 0m 0s) Loss: 0.0464(0.1143) \n",
      "Epoch: [3][0/366] Elapsed 0m 1s (remain 11m 14s) Loss: 0.1029(0.1029) Grad: 245852.7812  LR: 0.00001000  \n",
      "Epoch: [3][20/366] Elapsed 0m 22s (remain 6m 15s) Loss: 0.0674(0.0925) Grad: 112748.8906  LR: 0.00000957  \n",
      "Epoch: [3][40/366] Elapsed 0m 43s (remain 5m 48s) Loss: 0.0743(0.0886) Grad: 91753.9688  LR: 0.00000914  \n",
      "Epoch: [3][60/366] Elapsed 1m 2s (remain 5m 12s) Loss: 0.0644(0.0833) Grad: 49524.2070  LR: 0.00000872  \n",
      "Epoch: [3][80/366] Elapsed 1m 22s (remain 4m 51s) Loss: 0.0544(0.0811) Grad: 50591.6680  LR: 0.00000829  \n",
      "Epoch: [3][100/366] Elapsed 1m 42s (remain 4m 28s) Loss: 0.0598(0.0807) Grad: 65211.3359  LR: 0.00000787  \n",
      "Epoch: [3][120/366] Elapsed 2m 3s (remain 4m 9s) Loss: 0.2065(0.0819) Grad: 569860.7500  LR: 0.00000746  \n",
      "Epoch: [3][140/366] Elapsed 2m 23s (remain 3m 48s) Loss: 0.1242(0.0815) Grad: 68175.4375  LR: 0.00000704  \n",
      "Epoch: [3][160/366] Elapsed 2m 44s (remain 3m 29s) Loss: 0.0702(0.0813) Grad: 38029.6484  LR: 0.00000664  \n",
      "Epoch: [3][180/366] Elapsed 3m 5s (remain 3m 9s) Loss: 0.0627(0.0804) Grad: 35915.3203  LR: 0.00000624  \n",
      "Epoch: [3][200/366] Elapsed 3m 24s (remain 2m 48s) Loss: 0.0709(0.0797) Grad: 37251.6719  LR: 0.00000584  \n",
      "Epoch: [3][220/366] Elapsed 3m 43s (remain 2m 26s) Loss: 0.0753(0.0791) Grad: 47809.4141  LR: 0.00000546  \n",
      "Epoch: [3][240/366] Elapsed 4m 4s (remain 2m 6s) Loss: 0.0898(0.0791) Grad: 47368.5898  LR: 0.00000508  \n",
      "Epoch: [3][260/366] Elapsed 4m 25s (remain 1m 46s) Loss: 0.0968(0.0788) Grad: 53860.6328  LR: 0.00000471  \n",
      "Epoch: [3][280/366] Elapsed 4m 45s (remain 1m 26s) Loss: 0.0872(0.0788) Grad: 19990.3809  LR: 0.00000435  \n",
      "Epoch: [3][300/366] Elapsed 5m 7s (remain 1m 6s) Loss: 0.1141(0.0789) Grad: 46688.6992  LR: 0.00000400  \n",
      "Epoch: [3][320/366] Elapsed 5m 25s (remain 0m 45s) Loss: 0.0735(0.0782) Grad: 28436.8145  LR: 0.00000367  \n",
      "Epoch: [3][340/366] Elapsed 5m 46s (remain 0m 25s) Loss: 0.0618(0.0784) Grad: 19066.6699  LR: 0.00000334  \n",
      "Epoch: [3][360/366] Elapsed 6m 9s (remain 0m 5s) Loss: 0.0978(0.0778) Grad: 42622.8789  LR: 0.00000303  \n",
      "Epoch: [3][365/366] Elapsed 6m 14s (remain 0m 0s) Loss: 0.1041(0.0780) Grad: 40703.5508  LR: 0.00000295  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 40s) Loss: 0.0712(0.0712) \n",
      "EVAL: [20/62] Elapsed 0m 28s (remain 0m 56s) Loss: 0.0789(0.1045) \n",
      "EVAL: [40/62] Elapsed 0m 54s (remain 0m 28s) Loss: 0.0966(0.1037) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0780  avg_val_loss: 0.1051  time: 455s\n",
      "Epoch 3 - Score: 0.4591  Scores: [0.48800976980298394, 0.449876876155688, 0.41473998080111657, 0.46435275598964876, 0.4797654701913428, 0.4576240322704085]\n",
      "Epoch 3 - Save Best Score: 0.4591 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 1m 19s (remain 0m 1s) Loss: 0.1006(0.1051) \n",
      "EVAL: [61/62] Elapsed 1m 20s (remain 0m 0s) Loss: 0.0861(0.1051) \n",
      "Epoch: [4][0/366] Elapsed 0m 1s (remain 7m 21s) Loss: 0.0673(0.0673) Grad: 120809.7031  LR: 0.00000294  \n",
      "Epoch: [4][20/366] Elapsed 0m 23s (remain 6m 21s) Loss: 0.0495(0.0651) Grad: 93917.3594  LR: 0.00000264  \n",
      "Epoch: [4][40/366] Elapsed 0m 43s (remain 5m 41s) Loss: 0.0425(0.0639) Grad: 76096.0781  LR: 0.00000236  \n",
      "Epoch: [4][60/366] Elapsed 1m 4s (remain 5m 21s) Loss: 0.0350(0.0630) Grad: 30480.9355  LR: 0.00000209  \n",
      "Epoch: [4][80/366] Elapsed 1m 25s (remain 5m 1s) Loss: 0.0420(0.0613) Grad: 30966.2383  LR: 0.00000183  \n",
      "Epoch: [4][100/366] Elapsed 1m 44s (remain 4m 33s) Loss: 0.0959(0.0609) Grad: 86406.6328  LR: 0.00000159  \n",
      "Epoch: [4][120/366] Elapsed 2m 4s (remain 4m 13s) Loss: 0.0648(0.0606) Grad: 49174.4297  LR: 0.00000137  \n",
      "Epoch: [4][140/366] Elapsed 2m 25s (remain 3m 52s) Loss: 0.0710(0.0608) Grad: 41804.5625  LR: 0.00000116  \n",
      "Epoch: [4][160/366] Elapsed 2m 46s (remain 3m 31s) Loss: 0.0760(0.0608) Grad: 68273.9688  LR: 0.00000097  \n",
      "Epoch: [4][180/366] Elapsed 3m 7s (remain 3m 11s) Loss: 0.0730(0.0609) Grad: 63790.8594  LR: 0.00000079  \n",
      "Epoch: [4][200/366] Elapsed 3m 28s (remain 2m 51s) Loss: 0.0539(0.0606) Grad: 43274.4141  LR: 0.00000063  \n",
      "Epoch: [4][220/366] Elapsed 3m 48s (remain 2m 29s) Loss: 0.0675(0.0610) Grad: 57488.2891  LR: 0.00000049  \n",
      "Epoch: [4][240/366] Elapsed 4m 7s (remain 2m 8s) Loss: 0.0555(0.0607) Grad: 33272.5859  LR: 0.00000037  \n",
      "Epoch: [4][260/366] Elapsed 4m 27s (remain 1m 47s) Loss: 0.0572(0.0606) Grad: 33194.5352  LR: 0.00000026  \n",
      "Epoch: [4][280/366] Elapsed 4m 47s (remain 1m 26s) Loss: 0.0530(0.0603) Grad: 46971.9570  LR: 0.00000017  \n",
      "Epoch: [4][300/366] Elapsed 5m 8s (remain 1m 6s) Loss: 0.0511(0.0603) Grad: 45600.7695  LR: 0.00000010  \n",
      "Epoch: [4][320/366] Elapsed 5m 28s (remain 0m 46s) Loss: 0.0460(0.0601) Grad: 39016.0430  LR: 0.00000005  \n",
      "Epoch: [4][340/366] Elapsed 5m 48s (remain 0m 25s) Loss: 0.0388(0.0603) Grad: 44202.8672  LR: 0.00000002  \n",
      "Epoch: [4][360/366] Elapsed 6m 8s (remain 0m 5s) Loss: 0.0404(0.0601) Grad: 40733.1328  LR: 0.00000000  \n",
      "Epoch: [4][365/366] Elapsed 6m 13s (remain 0m 0s) Loss: 0.0628(0.0601) Grad: 37862.0352  LR: 0.00000000  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 42s) Loss: 0.0706(0.0706) \n",
      "EVAL: [20/62] Elapsed 0m 28s (remain 0m 56s) Loss: 0.0776(0.1051) \n",
      "EVAL: [40/62] Elapsed 0m 54s (remain 0m 28s) Loss: 0.0975(0.1054) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0601  avg_val_loss: 0.1062  time: 454s\n",
      "Epoch 4 - Score: 0.4617  Scores: [0.4890047923137198, 0.45402126197800524, 0.41779939269316174, 0.4674607308666758, 0.4819860024291688, 0.4597733960511384]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 1m 20s (remain 0m 1s) Loss: 0.1077(0.1063) \n",
      "EVAL: [61/62] Elapsed 1m 20s (remain 0m 0s) Loss: 0.0663(0.1062) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 result ==========\n",
      "Score: 0.4591  Scores: [0.48800976980298394, 0.449876876155688, 0.41473998080111657, 0.46435275598964876, 0.4797654701913428, 0.4576240322704085]\n",
      "========== fold: 1 training ==========\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "LSGRobertaConfig {\n",
      "  \"_name_or_path\": \"/root/autodl-tmp/fb3/inputs/common-nlp-tokenizer/model_tokenizer/tiny/lsg-roberta-large/\",\n",
      "  \"adaptive\": true,\n",
      "  \"architectures\": [\n",
      "    \"LSGRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoModel\": \"modeling_lsg_roberta.LSGRobertaModel\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_lsg_roberta.LSGRobertaForCausalLM\",\n",
      "    \"AutoModelForMaskedLM\": \"modeling_lsg_roberta.LSGRobertaForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modeling_lsg_roberta.LSGRobertaForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modeling_lsg_roberta.LSGRobertaForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modeling_lsg_roberta.LSGRobertaForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modeling_lsg_roberta.LSGRobertaForTokenClassification\"\n",
      "  },\n",
      "  \"base_model_prefix\": \"lsg\",\n",
      "  \"block_size\": 128,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"lsh_num_pre_rounds\": 1,\n",
      "  \"mask_first_token\": false,\n",
      "  \"max_position_embeddings\": 1538,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_global_tokens\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pool_with_global\": true,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sparse_block_size\": 128,\n",
      "  \"sparsity_factor\": 2,\n",
      "  \"sparsity_type\": \"norm\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Some weights of the model checkpoint at /root/autodl-tmp/fb3/inputs/common-nlp-tokenizer/model_tokenizer/tiny/lsg-roberta-large/ were not used when initializing LSGRobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing LSGRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LSGRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LSGRobertaModel were not initialized from the model checkpoint at /root/autodl-tmp/fb3/inputs/common-nlp-tokenizer/model_tokenizer/tiny/lsg-roberta-large/ and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/366] Elapsed 0m 2s (remain 12m 52s) Loss: 2.6034(2.6034) Grad: inf  LR: 0.00002000  \n",
      "Epoch: [1][20/366] Elapsed 0m 21s (remain 5m 54s) Loss: 0.1593(1.2056) Grad: 77986.7812  LR: 0.00001999  \n",
      "Epoch: [1][40/366] Elapsed 0m 40s (remain 5m 18s) Loss: 0.1441(0.6957) Grad: 149316.4219  LR: 0.00001996  \n",
      "Epoch: [1][60/366] Elapsed 1m 0s (remain 5m 3s) Loss: 0.1071(0.5175) Grad: 67985.0703  LR: 0.00001991  \n",
      "Epoch: [1][80/366] Elapsed 1m 23s (remain 4m 52s) Loss: 0.2051(0.4453) Grad: 216523.3438  LR: 0.00001985  \n",
      "Epoch: [1][100/366] Elapsed 1m 43s (remain 4m 32s) Loss: 0.1969(0.3900) Grad: 81942.7031  LR: 0.00001977  \n",
      "Epoch: [1][120/366] Elapsed 2m 4s (remain 4m 11s) Loss: 0.1523(0.3486) Grad: 90648.4375  LR: 0.00001967  \n",
      "Epoch: [1][140/366] Elapsed 2m 23s (remain 3m 49s) Loss: 0.1366(0.3199) Grad: 60521.3477  LR: 0.00001955  \n",
      "Epoch: [1][160/366] Elapsed 2m 43s (remain 3m 28s) Loss: 0.1491(0.2983) Grad: 46702.0586  LR: 0.00001941  \n",
      "Epoch: [1][180/366] Elapsed 3m 3s (remain 3m 7s) Loss: 0.1321(0.2800) Grad: 22806.6699  LR: 0.00001926  \n",
      "Epoch: [1][200/366] Elapsed 3m 20s (remain 2m 44s) Loss: 0.1159(0.2643) Grad: 23123.5488  LR: 0.00001909  \n",
      "Epoch: [1][220/366] Elapsed 3m 41s (remain 2m 25s) Loss: 0.1134(0.2519) Grad: 43830.9023  LR: 0.00001890  \n",
      "Epoch: [1][240/366] Elapsed 4m 1s (remain 2m 5s) Loss: 0.1183(0.2431) Grad: 28364.0254  LR: 0.00001870  \n",
      "Epoch: [1][260/366] Elapsed 4m 23s (remain 1m 45s) Loss: 0.0892(0.2333) Grad: 28081.9844  LR: 0.00001848  \n",
      "Epoch: [1][280/366] Elapsed 4m 42s (remain 1m 25s) Loss: 0.0802(0.2263) Grad: 23269.1680  LR: 0.00001824  \n",
      "Epoch: [1][300/366] Elapsed 5m 2s (remain 1m 5s) Loss: 0.0950(0.2198) Grad: 9984.0801  LR: 0.00001799  \n",
      "Epoch: [1][320/366] Elapsed 5m 22s (remain 0m 45s) Loss: 0.1903(0.2142) Grad: 12552.9609  LR: 0.00001773  \n",
      "Epoch: [1][340/366] Elapsed 5m 42s (remain 0m 25s) Loss: 0.0992(0.2108) Grad: 13495.3086  LR: 0.00001745  \n",
      "Epoch: [1][360/366] Elapsed 6m 4s (remain 0m 5s) Loss: 0.1300(0.2057) Grad: 17095.4375  LR: 0.00001716  \n",
      "Epoch: [1][365/366] Elapsed 6m 9s (remain 0m 0s) Loss: 0.2204(0.2047) Grad: 20903.6816  LR: 0.00001708  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 23s) Loss: 0.1120(0.1120) \n",
      "EVAL: [20/62] Elapsed 0m 28s (remain 0m 56s) Loss: 0.1484(0.1290) \n",
      "EVAL: [40/62] Elapsed 0m 54s (remain 0m 28s) Loss: 0.1102(0.1257) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.2047  avg_val_loss: 0.1232  time: 452s\n",
      "Epoch 1 - Score: 0.4983  Scores: [0.5467130909578175, 0.5013520968202666, 0.45282066050227476, 0.49961139011930483, 0.5141662413613545, 0.4749672983527204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 1m 22s (remain 0m 1s) Loss: 0.1014(0.1233) \n",
      "EVAL: [61/62] Elapsed 1m 22s (remain 0m 0s) Loss: 0.0458(0.1232) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Save Best Score: 0.4983 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/366] Elapsed 0m 1s (remain 8m 20s) Loss: 0.0612(0.0612) Grad: 107059.4922  LR: 0.00001707  \n",
      "Epoch: [2][20/366] Elapsed 0m 23s (remain 6m 18s) Loss: 0.0671(0.1172) Grad: 48043.7969  LR: 0.00001676  \n",
      "Epoch: [2][40/366] Elapsed 0m 42s (remain 5m 38s) Loss: 0.1113(0.1097) Grad: 55183.1367  LR: 0.00001644  \n",
      "Epoch: [2][60/366] Elapsed 1m 5s (remain 5m 27s) Loss: 0.0683(0.1080) Grad: 48561.8125  LR: 0.00001610  \n",
      "Epoch: [2][80/366] Elapsed 1m 26s (remain 5m 3s) Loss: 0.1022(0.1054) Grad: 42344.0625  LR: 0.00001576  \n",
      "Epoch: [2][100/366] Elapsed 1m 46s (remain 4m 38s) Loss: 0.0948(0.1032) Grad: 59557.8516  LR: 0.00001540  \n",
      "Epoch: [2][120/366] Elapsed 2m 5s (remain 4m 14s) Loss: 0.2028(0.1044) Grad: 110316.5156  LR: 0.00001504  \n",
      "Epoch: [2][140/366] Elapsed 2m 27s (remain 3m 54s) Loss: 0.0677(0.1047) Grad: 53880.8125  LR: 0.00001466  \n",
      "Epoch: [2][160/366] Elapsed 2m 47s (remain 3m 33s) Loss: 0.0746(0.1040) Grad: 38056.3203  LR: 0.00001428  \n",
      "Epoch: [2][180/366] Elapsed 3m 9s (remain 3m 14s) Loss: 0.0659(0.1033) Grad: 38098.5508  LR: 0.00001389  \n",
      "Epoch: [2][200/366] Elapsed 3m 30s (remain 2m 52s) Loss: 0.0776(0.1034) Grad: 39674.2695  LR: 0.00001349  \n",
      "Epoch: [2][220/366] Elapsed 3m 51s (remain 2m 31s) Loss: 0.0640(0.1025) Grad: 33530.7578  LR: 0.00001309  \n",
      "Epoch: [2][240/366] Elapsed 4m 10s (remain 2m 9s) Loss: 0.0613(0.1013) Grad: 47843.6562  LR: 0.00001268  \n",
      "Epoch: [2][260/366] Elapsed 4m 29s (remain 1m 48s) Loss: 0.1205(0.1014) Grad: 67369.6094  LR: 0.00001226  \n",
      "Epoch: [2][280/366] Elapsed 4m 48s (remain 1m 27s) Loss: 0.1084(0.1008) Grad: 52404.0898  LR: 0.00001184  \n",
      "Epoch: [2][300/366] Elapsed 5m 7s (remain 1m 6s) Loss: 0.1425(0.1006) Grad: 60142.0508  LR: 0.00001142  \n",
      "Epoch: [2][320/366] Elapsed 5m 28s (remain 0m 46s) Loss: 0.1043(0.1003) Grad: 63729.7617  LR: 0.00001099  \n",
      "Epoch: [2][340/366] Elapsed 5m 50s (remain 0m 25s) Loss: 0.0782(0.1001) Grad: 33958.6367  LR: 0.00001057  \n",
      "Epoch: [2][360/366] Elapsed 6m 10s (remain 0m 5s) Loss: 0.0923(0.0996) Grad: 77326.3906  LR: 0.00001014  \n",
      "Epoch: [2][365/366] Elapsed 6m 15s (remain 0m 0s) Loss: 0.1617(0.0998) Grad: 77354.7266  LR: 0.00001003  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 23s) Loss: 0.0904(0.0904) \n",
      "EVAL: [20/62] Elapsed 0m 28s (remain 0m 56s) Loss: 0.0996(0.1085) \n",
      "EVAL: [40/62] Elapsed 0m 54s (remain 0m 28s) Loss: 0.1084(0.1102) \n",
      "EVAL: [60/62] Elapsed 1m 22s (remain 0m 1s) Loss: 0.0969(0.1105) \n",
      "EVAL: [61/62] Elapsed 1m 22s (remain 0m 0s) Loss: 0.0660(0.1105) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0998  avg_val_loss: 0.1105  time: 458s\n",
      "Epoch 2 - Score: 0.4714  Scores: [0.5161704733289567, 0.4589547357915421, 0.4328601476292719, 0.4794403192568423, 0.478749265539737, 0.462518629759681]\n",
      "Epoch 2 - Save Best Score: 0.4714 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/366] Elapsed 0m 1s (remain 7m 56s) Loss: 0.0575(0.0575) Grad: inf  LR: 0.00001001  \n",
      "Epoch: [3][20/366] Elapsed 0m 21s (remain 5m 57s) Loss: 0.0815(0.0809) Grad: 81214.8828  LR: 0.00000958  \n",
      "Epoch: [3][40/366] Elapsed 0m 41s (remain 5m 29s) Loss: 0.0744(0.0785) Grad: 90250.4531  LR: 0.00000916  \n",
      "Epoch: [3][60/366] Elapsed 1m 2s (remain 5m 11s) Loss: 0.0768(0.0792) Grad: 50576.1289  LR: 0.00000873  \n",
      "Epoch: [3][80/366] Elapsed 1m 21s (remain 4m 47s) Loss: 0.0562(0.0795) Grad: 60141.0703  LR: 0.00000831  \n",
      "Epoch: [3][100/366] Elapsed 1m 41s (remain 4m 26s) Loss: 0.0999(0.0811) Grad: 46836.8047  LR: 0.00000789  \n",
      "Epoch: [3][120/366] Elapsed 2m 2s (remain 4m 8s) Loss: 0.0999(0.0815) Grad: 28930.6348  LR: 0.00000747  \n",
      "Epoch: [3][140/366] Elapsed 2m 22s (remain 3m 48s) Loss: 0.1026(0.0815) Grad: 27975.7109  LR: 0.00000706  \n",
      "Epoch: [3][160/366] Elapsed 2m 43s (remain 3m 28s) Loss: 0.0772(0.0810) Grad: 34808.2500  LR: 0.00000665  \n",
      "Epoch: [3][180/366] Elapsed 3m 3s (remain 3m 7s) Loss: 0.1050(0.0812) Grad: 30638.6445  LR: 0.00000625  \n",
      "Epoch: [3][200/366] Elapsed 3m 25s (remain 2m 49s) Loss: 0.0686(0.0810) Grad: 18474.4551  LR: 0.00000586  \n",
      "Epoch: [3][220/366] Elapsed 3m 46s (remain 2m 28s) Loss: 0.0891(0.0812) Grad: 34389.6680  LR: 0.00000547  \n",
      "Epoch: [3][240/366] Elapsed 4m 6s (remain 2m 7s) Loss: 0.1055(0.0803) Grad: 23826.6562  LR: 0.00000509  \n",
      "Epoch: [3][260/366] Elapsed 4m 23s (remain 1m 46s) Loss: 0.0883(0.0801) Grad: 21928.4180  LR: 0.00000472  \n",
      "Epoch: [3][280/366] Elapsed 4m 44s (remain 1m 25s) Loss: 0.0610(0.0804) Grad: 25989.8594  LR: 0.00000437  \n",
      "Epoch: [3][300/366] Elapsed 5m 5s (remain 1m 5s) Loss: 0.0686(0.0803) Grad: 42914.0781  LR: 0.00000402  \n",
      "Epoch: [3][320/366] Elapsed 5m 26s (remain 0m 45s) Loss: 0.1189(0.0800) Grad: 35298.1562  LR: 0.00000368  \n",
      "Epoch: [3][340/366] Elapsed 5m 46s (remain 0m 25s) Loss: 0.0750(0.0802) Grad: 23504.1191  LR: 0.00000335  \n",
      "Epoch: [3][360/366] Elapsed 6m 7s (remain 0m 5s) Loss: 0.0613(0.0801) Grad: 26461.2734  LR: 0.00000304  \n",
      "Epoch: [3][365/366] Elapsed 6m 12s (remain 0m 0s) Loss: 0.0856(0.0802) Grad: 36080.3438  LR: 0.00000296  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 26s) Loss: 0.0949(0.0949) \n",
      "EVAL: [20/62] Elapsed 0m 28s (remain 0m 56s) Loss: 0.1020(0.1081) \n",
      "EVAL: [40/62] Elapsed 0m 54s (remain 0m 28s) Loss: 0.1003(0.1088) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0802  avg_val_loss: 0.1095  time: 455s\n",
      "Epoch 3 - Score: 0.4695  Scores: [0.5023983912980233, 0.4510255634138228, 0.4355078752192412, 0.47155232491980587, 0.4818898174888112, 0.4748614751008684]\n",
      "Epoch 3 - Save Best Score: 0.4695 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 1m 22s (remain 0m 1s) Loss: 0.1011(0.1095) \n",
      "EVAL: [61/62] Elapsed 1m 22s (remain 0m 0s) Loss: 0.0975(0.1095) \n",
      "Epoch: [4][0/366] Elapsed 0m 2s (remain 12m 38s) Loss: 0.0763(0.0763) Grad: inf  LR: 0.00000295  \n",
      "Epoch: [4][20/366] Elapsed 0m 22s (remain 6m 6s) Loss: 0.0592(0.0708) Grad: 118672.0859  LR: 0.00000265  \n",
      "Epoch: [4][40/366] Elapsed 0m 42s (remain 5m 35s) Loss: 0.0482(0.0681) Grad: 70350.7031  LR: 0.00000237  \n",
      "Epoch: [4][60/366] Elapsed 1m 2s (remain 5m 14s) Loss: 0.0694(0.0650) Grad: 65947.5703  LR: 0.00000210  \n",
      "Epoch: [4][80/366] Elapsed 1m 25s (remain 5m 1s) Loss: 0.0534(0.0644) Grad: 164223.2656  LR: 0.00000184  \n",
      "Epoch: [4][100/366] Elapsed 1m 47s (remain 4m 41s) Loss: 0.0661(0.0642) Grad: 102581.3750  LR: 0.00000160  \n",
      "Epoch: [4][120/366] Elapsed 2m 8s (remain 4m 20s) Loss: 0.0593(0.0639) Grad: 130870.6406  LR: 0.00000138  \n",
      "Epoch: [4][140/366] Elapsed 2m 29s (remain 3m 59s) Loss: 0.0705(0.0634) Grad: 99549.3984  LR: 0.00000117  \n",
      "Epoch: [4][160/366] Elapsed 2m 48s (remain 3m 34s) Loss: 0.0604(0.0638) Grad: 31212.8301  LR: 0.00000098  \n",
      "Epoch: [4][180/366] Elapsed 3m 8s (remain 3m 13s) Loss: 0.0516(0.0631) Grad: 154138.9844  LR: 0.00000080  \n",
      "Epoch: [4][200/366] Elapsed 3m 27s (remain 2m 50s) Loss: 0.0588(0.0627) Grad: 44213.2305  LR: 0.00000064  \n",
      "Epoch: [4][220/366] Elapsed 3m 47s (remain 2m 29s) Loss: 0.0648(0.0622) Grad: 32756.8398  LR: 0.00000050  \n",
      "Epoch: [4][240/366] Elapsed 4m 7s (remain 2m 8s) Loss: 0.0735(0.0621) Grad: 43459.4414  LR: 0.00000037  \n",
      "Epoch: [4][260/366] Elapsed 4m 27s (remain 1m 47s) Loss: 0.0514(0.0624) Grad: 20479.1016  LR: 0.00000027  \n",
      "Epoch: [4][280/366] Elapsed 4m 48s (remain 1m 27s) Loss: 0.0689(0.0626) Grad: 47591.1523  LR: 0.00000018  \n",
      "Epoch: [4][300/366] Elapsed 5m 8s (remain 1m 6s) Loss: 0.0622(0.0622) Grad: 51941.5234  LR: 0.00000011  \n",
      "Epoch: [4][320/366] Elapsed 5m 28s (remain 0m 45s) Loss: 0.0538(0.0622) Grad: 43467.5391  LR: 0.00000005  \n",
      "Epoch: [4][340/366] Elapsed 5m 49s (remain 0m 25s) Loss: 0.0448(0.0621) Grad: 28844.4277  LR: 0.00000002  \n",
      "Epoch: [4][360/366] Elapsed 6m 8s (remain 0m 5s) Loss: 0.0417(0.0623) Grad: 30041.7402  LR: 0.00000000  \n",
      "Epoch: [4][365/366] Elapsed 6m 13s (remain 0m 0s) Loss: 0.0771(0.0622) Grad: 31101.5566  LR: 0.00000000  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 22s) Loss: 0.0953(0.0953) \n",
      "EVAL: [20/62] Elapsed 0m 28s (remain 0m 56s) Loss: 0.1069(0.1084) \n",
      "EVAL: [40/62] Elapsed 0m 54s (remain 0m 28s) Loss: 0.0982(0.1076) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0622  avg_val_loss: 0.1081  time: 456s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 1m 22s (remain 0m 1s) Loss: 0.1021(0.1081) \n",
      "EVAL: [61/62] Elapsed 1m 22s (remain 0m 0s) Loss: 0.0989(0.1081) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Score: 0.4663  Scores: [0.5021884360711852, 0.4555507010405653, 0.42721438551001023, 0.4652661169739467, 0.4822360288131827, 0.4652723727226996]\n",
      "Epoch 4 - Save Best Score: 0.4663 Model\n",
      "========== fold: 1 result ==========\n",
      "Score: 0.4663  Scores: [0.5021884360711852, 0.4555507010405653, 0.42721438551001023, 0.4652661169739467, 0.4822360288131827, 0.4652723727226996]\n",
      "========== fold: 2 training ==========\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "LSGRobertaConfig {\n",
      "  \"_name_or_path\": \"/root/autodl-tmp/fb3/inputs/common-nlp-tokenizer/model_tokenizer/tiny/lsg-roberta-large/\",\n",
      "  \"adaptive\": true,\n",
      "  \"architectures\": [\n",
      "    \"LSGRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoModel\": \"modeling_lsg_roberta.LSGRobertaModel\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_lsg_roberta.LSGRobertaForCausalLM\",\n",
      "    \"AutoModelForMaskedLM\": \"modeling_lsg_roberta.LSGRobertaForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modeling_lsg_roberta.LSGRobertaForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modeling_lsg_roberta.LSGRobertaForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modeling_lsg_roberta.LSGRobertaForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modeling_lsg_roberta.LSGRobertaForTokenClassification\"\n",
      "  },\n",
      "  \"base_model_prefix\": \"lsg\",\n",
      "  \"block_size\": 128,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"lsh_num_pre_rounds\": 1,\n",
      "  \"mask_first_token\": false,\n",
      "  \"max_position_embeddings\": 1538,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_global_tokens\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pool_with_global\": true,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sparse_block_size\": 128,\n",
      "  \"sparsity_factor\": 2,\n",
      "  \"sparsity_type\": \"norm\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Some weights of the model checkpoint at /root/autodl-tmp/fb3/inputs/common-nlp-tokenizer/model_tokenizer/tiny/lsg-roberta-large/ were not used when initializing LSGRobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing LSGRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LSGRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LSGRobertaModel were not initialized from the model checkpoint at /root/autodl-tmp/fb3/inputs/common-nlp-tokenizer/model_tokenizer/tiny/lsg-roberta-large/ and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/366] Elapsed 0m 1s (remain 9m 32s) Loss: 2.8474(2.8474) Grad: inf  LR: 0.00002000  \n",
      "Epoch: [1][20/366] Elapsed 0m 22s (remain 6m 9s) Loss: 0.2772(1.0829) Grad: 153385.0156  LR: 0.00001999  \n",
      "Epoch: [1][40/366] Elapsed 0m 44s (remain 5m 54s) Loss: 0.0938(0.6485) Grad: 17450.7480  LR: 0.00001996  \n",
      "Epoch: [1][60/366] Elapsed 1m 7s (remain 5m 35s) Loss: 0.2656(0.4884) Grad: 110228.5312  LR: 0.00001991  \n",
      "Epoch: [1][80/366] Elapsed 1m 28s (remain 5m 12s) Loss: 0.1582(0.4089) Grad: 38478.4062  LR: 0.00001985  \n",
      "Epoch: [1][100/366] Elapsed 1m 49s (remain 4m 46s) Loss: 0.1849(0.3591) Grad: 30988.8574  LR: 0.00001977  \n",
      "Epoch: [1][120/366] Elapsed 2m 9s (remain 4m 21s) Loss: 0.1821(0.3233) Grad: 26948.2363  LR: 0.00001967  \n",
      "Epoch: [1][140/366] Elapsed 2m 31s (remain 4m 1s) Loss: 0.1353(0.3012) Grad: 31192.8496  LR: 0.00001955  \n",
      "Epoch: [1][160/366] Elapsed 2m 50s (remain 3m 37s) Loss: 0.1338(0.2854) Grad: 17400.1289  LR: 0.00001941  \n",
      "Epoch: [1][180/366] Elapsed 3m 11s (remain 3m 15s) Loss: 0.1424(0.2682) Grad: 34369.6719  LR: 0.00001926  \n",
      "Epoch: [1][200/366] Elapsed 3m 32s (remain 2m 54s) Loss: 0.2298(0.2564) Grad: 34751.1875  LR: 0.00001909  \n",
      "Epoch: [1][220/366] Elapsed 3m 52s (remain 2m 32s) Loss: 0.0876(0.2444) Grad: 8350.3281  LR: 0.00001890  \n",
      "Epoch: [1][240/366] Elapsed 4m 11s (remain 2m 10s) Loss: 0.1218(0.2344) Grad: 16209.3516  LR: 0.00001870  \n",
      "Epoch: [1][260/366] Elapsed 4m 33s (remain 1m 49s) Loss: 0.2197(0.2288) Grad: 26530.1426  LR: 0.00001848  \n",
      "Epoch: [1][280/366] Elapsed 4m 54s (remain 1m 29s) Loss: 0.1346(0.2227) Grad: 15629.1543  LR: 0.00001824  \n",
      "Epoch: [1][300/366] Elapsed 5m 13s (remain 1m 7s) Loss: 0.1493(0.2164) Grad: 31046.3066  LR: 0.00001799  \n",
      "Epoch: [1][320/366] Elapsed 5m 34s (remain 0m 46s) Loss: 0.1536(0.2119) Grad: 17702.5117  LR: 0.00001773  \n",
      "Epoch: [1][340/366] Elapsed 5m 53s (remain 0m 25s) Loss: 0.1459(0.2059) Grad: 32579.0195  LR: 0.00001745  \n",
      "Epoch: [1][360/366] Elapsed 6m 14s (remain 0m 5s) Loss: 0.0590(0.2006) Grad: 10272.0859  LR: 0.00001715  \n",
      "Epoch: [1][365/366] Elapsed 6m 19s (remain 0m 0s) Loss: 0.0888(0.1992) Grad: 22469.5312  LR: 0.00001708  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 9s) Loss: 0.1485(0.1485) \n",
      "EVAL: [20/62] Elapsed 0m 26s (remain 0m 51s) Loss: 0.0970(0.1169) \n",
      "EVAL: [40/62] Elapsed 0m 51s (remain 0m 26s) Loss: 0.1200(0.1151) \n",
      "EVAL: [60/62] Elapsed 1m 16s (remain 0m 1s) Loss: 0.0888(0.1146) \n",
      "EVAL: [61/62] Elapsed 1m 17s (remain 0m 0s) Loss: 0.1647(0.1147) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1992  avg_val_loss: 0.1147  time: 457s\n",
      "Epoch 1 - Score: 0.4806  Scores: [0.5048764213380247, 0.48005302089817686, 0.4310334409137284, 0.48358612447330074, 0.5074383406335587, 0.47640331055001806]\n",
      "Epoch 1 - Save Best Score: 0.4806 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/366] Elapsed 0m 1s (remain 11m 32s) Loss: 0.0836(0.0836) Grad: inf  LR: 0.00001706  \n",
      "Epoch: [2][20/366] Elapsed 0m 22s (remain 6m 13s) Loss: 0.1505(0.1280) Grad: 82463.6641  LR: 0.00001675  \n",
      "Epoch: [2][40/366] Elapsed 0m 43s (remain 5m 45s) Loss: 0.0861(0.1129) Grad: 100359.4297  LR: 0.00001643  \n",
      "Epoch: [2][60/366] Elapsed 1m 3s (remain 5m 18s) Loss: 0.1166(0.1082) Grad: 88511.6641  LR: 0.00001610  \n",
      "Epoch: [2][80/366] Elapsed 1m 23s (remain 4m 52s) Loss: 0.0774(0.1052) Grad: 95481.3594  LR: 0.00001575  \n",
      "Epoch: [2][100/366] Elapsed 1m 44s (remain 4m 33s) Loss: 0.0684(0.1024) Grad: 85932.0625  LR: 0.00001540  \n",
      "Epoch: [2][120/366] Elapsed 2m 4s (remain 4m 12s) Loss: 0.1382(0.1028) Grad: 102850.2812  LR: 0.00001503  \n",
      "Epoch: [2][140/366] Elapsed 2m 25s (remain 3m 51s) Loss: 0.0835(0.1021) Grad: 160546.2656  LR: 0.00001466  \n",
      "Epoch: [2][160/366] Elapsed 2m 45s (remain 3m 30s) Loss: 0.0928(0.1008) Grad: 38087.4570  LR: 0.00001427  \n",
      "Epoch: [2][180/366] Elapsed 3m 7s (remain 3m 11s) Loss: 0.1007(0.0994) Grad: 33793.9219  LR: 0.00001388  \n",
      "Epoch: [2][200/366] Elapsed 3m 28s (remain 2m 50s) Loss: 0.1307(0.1000) Grad: 27122.3418  LR: 0.00001348  \n",
      "Epoch: [2][220/366] Elapsed 3m 47s (remain 2m 29s) Loss: 0.0721(0.0990) Grad: 19359.7559  LR: 0.00001308  \n",
      "Epoch: [2][240/366] Elapsed 4m 7s (remain 2m 8s) Loss: 0.1258(0.0987) Grad: 17700.2871  LR: 0.00001267  \n",
      "Epoch: [2][260/366] Elapsed 4m 29s (remain 1m 48s) Loss: 0.1357(0.0994) Grad: 15118.5820  LR: 0.00001225  \n",
      "Epoch: [2][280/366] Elapsed 4m 48s (remain 1m 27s) Loss: 0.1658(0.1000) Grad: 22376.4141  LR: 0.00001183  \n",
      "Epoch: [2][300/366] Elapsed 5m 9s (remain 1m 6s) Loss: 0.0714(0.0997) Grad: 14513.1201  LR: 0.00001141  \n",
      "Epoch: [2][320/366] Elapsed 5m 30s (remain 0m 46s) Loss: 0.1054(0.0997) Grad: 8787.5137  LR: 0.00001098  \n",
      "Epoch: [2][340/366] Elapsed 5m 50s (remain 0m 25s) Loss: 0.1151(0.1000) Grad: 13665.9336  LR: 0.00001056  \n",
      "Epoch: [2][360/366] Elapsed 6m 10s (remain 0m 5s) Loss: 0.0772(0.0999) Grad: 6732.3262  LR: 0.00001013  \n",
      "Epoch: [2][365/366] Elapsed 6m 16s (remain 0m 0s) Loss: 0.1018(0.0999) Grad: 16837.8887  LR: 0.00001002  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 7s) Loss: 0.1456(0.1456) \n",
      "EVAL: [20/62] Elapsed 0m 26s (remain 0m 51s) Loss: 0.1015(0.1169) \n",
      "EVAL: [40/62] Elapsed 0m 51s (remain 0m 26s) Loss: 0.1138(0.1148) \n",
      "EVAL: [60/62] Elapsed 1m 16s (remain 0m 1s) Loss: 0.1007(0.1160) \n",
      "EVAL: [61/62] Elapsed 1m 17s (remain 0m 0s) Loss: 0.1088(0.1160) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0999  avg_val_loss: 0.1160  time: 454s\n",
      "Epoch 2 - Score: 0.4835  Scores: [0.5016467486645255, 0.4678615699832894, 0.43810574927779267, 0.49082275983285856, 0.5212211716638004, 0.48124613828360385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/366] Elapsed 0m 1s (remain 10m 29s) Loss: 0.0599(0.0599) Grad: 115961.7344  LR: 0.00001000  \n",
      "Epoch: [3][20/366] Elapsed 0m 23s (remain 6m 23s) Loss: 0.0555(0.0798) Grad: 51371.7266  LR: 0.00000957  \n",
      "Epoch: [3][40/366] Elapsed 0m 43s (remain 5m 41s) Loss: 0.0871(0.0801) Grad: 123604.2188  LR: 0.00000914  \n",
      "Epoch: [3][60/366] Elapsed 1m 2s (remain 5m 12s) Loss: 0.0763(0.0803) Grad: 116414.3438  LR: 0.00000872  \n",
      "Epoch: [3][80/366] Elapsed 1m 23s (remain 4m 54s) Loss: 0.0966(0.0797) Grad: 102455.5938  LR: 0.00000829  \n",
      "Epoch: [3][100/366] Elapsed 1m 45s (remain 4m 36s) Loss: 0.0660(0.0785) Grad: 46876.4648  LR: 0.00000787  \n",
      "Epoch: [3][120/366] Elapsed 2m 6s (remain 4m 15s) Loss: 0.0639(0.0790) Grad: 28410.2461  LR: 0.00000746  \n",
      "Epoch: [3][140/366] Elapsed 2m 26s (remain 3m 54s) Loss: 0.0905(0.0799) Grad: 46145.7695  LR: 0.00000704  \n",
      "Epoch: [3][160/366] Elapsed 2m 49s (remain 3m 35s) Loss: 0.0720(0.0799) Grad: 37023.7031  LR: 0.00000664  \n",
      "Epoch: [3][180/366] Elapsed 3m 8s (remain 3m 12s) Loss: 0.0805(0.0789) Grad: 45003.3984  LR: 0.00000624  \n",
      "Epoch: [3][200/366] Elapsed 3m 29s (remain 2m 51s) Loss: 0.1267(0.0789) Grad: 59490.7930  LR: 0.00000584  \n",
      "Epoch: [3][220/366] Elapsed 3m 49s (remain 2m 30s) Loss: 0.0824(0.0793) Grad: 35658.2695  LR: 0.00000546  \n",
      "Epoch: [3][240/366] Elapsed 4m 10s (remain 2m 9s) Loss: 0.0738(0.0795) Grad: 32953.3750  LR: 0.00000508  \n",
      "Epoch: [3][260/366] Elapsed 4m 30s (remain 1m 48s) Loss: 0.0980(0.0795) Grad: 73063.2656  LR: 0.00000471  \n",
      "Epoch: [3][280/366] Elapsed 4m 51s (remain 1m 28s) Loss: 0.0841(0.0796) Grad: 56540.1367  LR: 0.00000435  \n",
      "Epoch: [3][300/366] Elapsed 5m 10s (remain 1m 7s) Loss: 0.0826(0.0791) Grad: 49745.7109  LR: 0.00000400  \n",
      "Epoch: [3][320/366] Elapsed 5m 30s (remain 0m 46s) Loss: 0.0645(0.0788) Grad: 35578.3359  LR: 0.00000367  \n",
      "Epoch: [3][340/366] Elapsed 5m 51s (remain 0m 25s) Loss: 0.0603(0.0784) Grad: 41240.3047  LR: 0.00000334  \n",
      "Epoch: [3][360/366] Elapsed 6m 10s (remain 0m 5s) Loss: 0.0954(0.0785) Grad: 41411.6562  LR: 0.00000303  \n",
      "Epoch: [3][365/366] Elapsed 6m 15s (remain 0m 0s) Loss: 0.1001(0.0784) Grad: 56049.9648  LR: 0.00000295  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 7s) Loss: 0.1343(0.1343) \n",
      "EVAL: [20/62] Elapsed 0m 26s (remain 0m 51s) Loss: 0.0887(0.1128) \n",
      "EVAL: [40/62] Elapsed 0m 51s (remain 0m 26s) Loss: 0.1010(0.1091) \n",
      "EVAL: [60/62] Elapsed 1m 16s (remain 0m 1s) Loss: 0.0961(0.1106) \n",
      "EVAL: [61/62] Elapsed 1m 17s (remain 0m 0s) Loss: 0.0974(0.1106) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0784  avg_val_loss: 0.1106  time: 453s\n",
      "Epoch 3 - Score: 0.4719  Scores: [0.49021717086416267, 0.4640692987093321, 0.4244595303300146, 0.48397979817210024, 0.49624336910996236, 0.4723036962719627]\n",
      "Epoch 3 - Save Best Score: 0.4719 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/366] Elapsed 0m 1s (remain 10m 19s) Loss: 0.0827(0.0827) Grad: 194514.1562  LR: 0.00000294  \n",
      "Epoch: [4][20/366] Elapsed 0m 22s (remain 6m 7s) Loss: 0.0484(0.0732) Grad: 86514.9688  LR: 0.00000264  \n",
      "Epoch: [4][40/366] Elapsed 0m 41s (remain 5m 31s) Loss: 0.0798(0.0717) Grad: 92546.0469  LR: 0.00000236  \n",
      "Epoch: [4][60/366] Elapsed 1m 2s (remain 5m 14s) Loss: 0.0600(0.0716) Grad: 47755.4961  LR: 0.00000209  \n",
      "Epoch: [4][80/366] Elapsed 1m 23s (remain 4m 55s) Loss: 0.0759(0.0710) Grad: 97034.2188  LR: 0.00000183  \n",
      "Epoch: [4][100/366] Elapsed 1m 44s (remain 4m 35s) Loss: 0.0897(0.0710) Grad: 90438.4141  LR: 0.00000159  \n",
      "Epoch: [4][120/366] Elapsed 2m 5s (remain 4m 14s) Loss: 0.0562(0.0701) Grad: 66291.7500  LR: 0.00000137  \n",
      "Epoch: [4][140/366] Elapsed 2m 25s (remain 3m 51s) Loss: 0.0686(0.0693) Grad: 38542.0898  LR: 0.00000116  \n",
      "Epoch: [4][160/366] Elapsed 2m 44s (remain 3m 29s) Loss: 0.0472(0.0693) Grad: 80411.5547  LR: 0.00000097  \n",
      "Epoch: [4][180/366] Elapsed 3m 3s (remain 3m 7s) Loss: 0.0839(0.0693) Grad: 96038.2656  LR: 0.00000079  \n",
      "Epoch: [4][200/366] Elapsed 3m 24s (remain 2m 47s) Loss: 0.0704(0.0691) Grad: 65712.7344  LR: 0.00000063  \n",
      "Epoch: [4][220/366] Elapsed 3m 45s (remain 2m 27s) Loss: 0.0666(0.0686) Grad: 83312.1719  LR: 0.00000049  \n",
      "Epoch: [4][240/366] Elapsed 4m 7s (remain 2m 8s) Loss: 0.0854(0.0681) Grad: 58531.4062  LR: 0.00000037  \n",
      "Epoch: [4][260/366] Elapsed 4m 28s (remain 1m 47s) Loss: 0.0977(0.0677) Grad: 111194.3906  LR: 0.00000026  \n",
      "Epoch: [4][280/366] Elapsed 4m 48s (remain 1m 27s) Loss: 0.0712(0.0676) Grad: 111362.2812  LR: 0.00000017  \n",
      "Epoch: [4][300/366] Elapsed 5m 9s (remain 1m 6s) Loss: 0.0580(0.0677) Grad: 65447.7578  LR: 0.00000010  \n",
      "Epoch: [4][320/366] Elapsed 5m 30s (remain 0m 46s) Loss: 0.0702(0.0679) Grad: 67102.2344  LR: 0.00000005  \n",
      "Epoch: [4][340/366] Elapsed 5m 52s (remain 0m 25s) Loss: 0.1328(0.0677) Grad: 91718.0703  LR: 0.00000002  \n",
      "Epoch: [4][360/366] Elapsed 6m 12s (remain 0m 5s) Loss: 0.0452(0.0681) Grad: 57700.2891  LR: 0.00000000  \n",
      "Epoch: [4][365/366] Elapsed 6m 18s (remain 0m 0s) Loss: 0.0962(0.0682) Grad: 92402.9375  LR: 0.00000000  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 6s) Loss: 0.1356(0.1356) \n",
      "EVAL: [20/62] Elapsed 0m 26s (remain 0m 51s) Loss: 0.0947(0.1102) \n",
      "EVAL: [40/62] Elapsed 0m 51s (remain 0m 26s) Loss: 0.1003(0.1076) \n",
      "EVAL: [60/62] Elapsed 1m 16s (remain 0m 1s) Loss: 0.0936(0.1092) \n",
      "EVAL: [61/62] Elapsed 1m 16s (remain 0m 0s) Loss: 0.1297(0.1093) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0682  avg_val_loss: 0.1093  time: 456s\n",
      "Epoch 4 - Score: 0.4688  Scores: [0.4908077140426496, 0.4584616953273456, 0.42302174243291835, 0.4745555359199424, 0.49821622138214083, 0.4675034422030983]\n",
      "Epoch 4 - Save Best Score: 0.4688 Model\n",
      "========== fold: 2 result ==========\n",
      "Score: 0.4688  Scores: [0.4908077140426496, 0.4584616953273456, 0.42302174243291835, 0.4745555359199424, 0.49821622138214083, 0.4675034422030983]\n",
      "========== fold: 3 training ==========\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "LSGRobertaConfig {\n",
      "  \"_name_or_path\": \"/root/autodl-tmp/fb3/inputs/common-nlp-tokenizer/model_tokenizer/tiny/lsg-roberta-large/\",\n",
      "  \"adaptive\": true,\n",
      "  \"architectures\": [\n",
      "    \"LSGRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoModel\": \"modeling_lsg_roberta.LSGRobertaModel\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_lsg_roberta.LSGRobertaForCausalLM\",\n",
      "    \"AutoModelForMaskedLM\": \"modeling_lsg_roberta.LSGRobertaForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modeling_lsg_roberta.LSGRobertaForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modeling_lsg_roberta.LSGRobertaForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modeling_lsg_roberta.LSGRobertaForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modeling_lsg_roberta.LSGRobertaForTokenClassification\"\n",
      "  },\n",
      "  \"base_model_prefix\": \"lsg\",\n",
      "  \"block_size\": 128,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"lsh_num_pre_rounds\": 1,\n",
      "  \"mask_first_token\": false,\n",
      "  \"max_position_embeddings\": 1538,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_global_tokens\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pool_with_global\": true,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sparse_block_size\": 128,\n",
      "  \"sparsity_factor\": 2,\n",
      "  \"sparsity_type\": \"norm\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Some weights of the model checkpoint at /root/autodl-tmp/fb3/inputs/common-nlp-tokenizer/model_tokenizer/tiny/lsg-roberta-large/ were not used when initializing LSGRobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing LSGRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LSGRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LSGRobertaModel were not initialized from the model checkpoint at /root/autodl-tmp/fb3/inputs/common-nlp-tokenizer/model_tokenizer/tiny/lsg-roberta-large/ and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/366] Elapsed 0m 1s (remain 8m 42s) Loss: 2.4858(2.4858) Grad: inf  LR: 0.00002000  \n",
      "Epoch: [1][20/366] Elapsed 0m 21s (remain 5m 55s) Loss: 0.2982(0.9732) Grad: 79796.3984  LR: 0.00001999  \n",
      "Epoch: [1][40/366] Elapsed 0m 42s (remain 5m 35s) Loss: 0.3802(0.6396) Grad: 105512.8359  LR: 0.00001996  \n",
      "Epoch: [1][60/366] Elapsed 1m 3s (remain 5m 18s) Loss: 0.1523(0.4986) Grad: 22185.8789  LR: 0.00001991  \n",
      "Epoch: [1][80/366] Elapsed 1m 23s (remain 4m 53s) Loss: 0.1712(0.4133) Grad: 56381.2305  LR: 0.00001985  \n",
      "Epoch: [1][100/366] Elapsed 1m 42s (remain 4m 29s) Loss: 0.1331(0.3633) Grad: 23590.2480  LR: 0.00001977  \n",
      "Epoch: [1][120/366] Elapsed 2m 3s (remain 4m 11s) Loss: 0.2231(0.3275) Grad: 40156.6797  LR: 0.00001967  \n",
      "Epoch: [1][140/366] Elapsed 2m 23s (remain 3m 49s) Loss: 0.1549(0.3068) Grad: 186607.7969  LR: 0.00001955  \n",
      "Epoch: [1][160/366] Elapsed 2m 43s (remain 3m 27s) Loss: 0.1490(0.2861) Grad: 46320.6758  LR: 0.00001941  \n",
      "Epoch: [1][180/366] Elapsed 3m 3s (remain 3m 7s) Loss: 0.1292(0.2673) Grad: 17174.5352  LR: 0.00001926  \n",
      "Epoch: [1][200/366] Elapsed 3m 24s (remain 2m 47s) Loss: 0.1181(0.2535) Grad: 33701.0195  LR: 0.00001909  \n",
      "Epoch: [1][220/366] Elapsed 3m 45s (remain 2m 27s) Loss: 0.1166(0.2422) Grad: 15344.0879  LR: 0.00001890  \n",
      "Epoch: [1][240/366] Elapsed 4m 7s (remain 2m 8s) Loss: 0.0989(0.2323) Grad: 18437.9375  LR: 0.00001870  \n",
      "Epoch: [1][260/366] Elapsed 4m 27s (remain 1m 47s) Loss: 0.0908(0.2242) Grad: 19235.7480  LR: 0.00001848  \n",
      "Epoch: [1][280/366] Elapsed 4m 48s (remain 1m 27s) Loss: 0.1032(0.2180) Grad: 11175.6494  LR: 0.00001824  \n",
      "Epoch: [1][300/366] Elapsed 5m 8s (remain 1m 6s) Loss: 0.1587(0.2124) Grad: 31835.5684  LR: 0.00001799  \n",
      "Epoch: [1][320/366] Elapsed 5m 26s (remain 0m 45s) Loss: 0.1452(0.2091) Grad: 22989.0371  LR: 0.00001773  \n",
      "Epoch: [1][340/366] Elapsed 5m 45s (remain 0m 25s) Loss: 0.1284(0.2051) Grad: 13839.4727  LR: 0.00001745  \n",
      "Epoch: [1][360/366] Elapsed 6m 5s (remain 0m 5s) Loss: 0.1591(0.2012) Grad: 27284.8926  LR: 0.00001715  \n",
      "Epoch: [1][365/366] Elapsed 6m 10s (remain 0m 0s) Loss: 0.1207(0.2001) Grad: 14409.7969  LR: 0.00001708  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 25s) Loss: 0.1029(0.1029) \n",
      "EVAL: [20/62] Elapsed 0m 28s (remain 0m 54s) Loss: 0.1119(0.1425) \n",
      "EVAL: [40/62] Elapsed 0m 55s (remain 0m 28s) Loss: 0.1660(0.1521) \n",
      "EVAL: [60/62] Elapsed 1m 22s (remain 0m 1s) Loss: 0.1262(0.1531) \n",
      "EVAL: [61/62] Elapsed 1m 22s (remain 0m 0s) Loss: 0.0591(0.1529) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.2001  avg_val_loss: 0.1529  time: 454s\n",
      "Epoch 1 - Score: 0.5581  Scores: [0.5273308714999403, 0.5475843815470336, 0.5215275948857212, 0.5280622762099592, 0.6169559107153975, 0.6070126791761387]\n",
      "Epoch 1 - Save Best Score: 0.5581 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/366] Elapsed 0m 1s (remain 9m 8s) Loss: 0.1033(0.1033) Grad: 250486.7969  LR: 0.00001706  \n",
      "Epoch: [2][20/366] Elapsed 0m 22s (remain 6m 5s) Loss: 0.0776(0.1343) Grad: 102519.8906  LR: 0.00001675  \n",
      "Epoch: [2][40/366] Elapsed 0m 43s (remain 5m 41s) Loss: 0.1099(0.1144) Grad: 183167.5312  LR: 0.00001643  \n",
      "Epoch: [2][60/366] Elapsed 1m 4s (remain 5m 23s) Loss: 0.0788(0.1077) Grad: 39579.7930  LR: 0.00001610  \n",
      "Epoch: [2][80/366] Elapsed 1m 25s (remain 4m 59s) Loss: 0.0582(0.1034) Grad: 40317.3477  LR: 0.00001575  \n",
      "Epoch: [2][100/366] Elapsed 1m 46s (remain 4m 38s) Loss: 0.0693(0.1026) Grad: 38740.3672  LR: 0.00001540  \n",
      "Epoch: [2][120/366] Elapsed 2m 6s (remain 4m 16s) Loss: 0.1026(0.1036) Grad: 52286.5391  LR: 0.00001503  \n",
      "Epoch: [2][140/366] Elapsed 2m 26s (remain 3m 54s) Loss: 0.0786(0.1014) Grad: 35736.5195  LR: 0.00001466  \n",
      "Epoch: [2][160/366] Elapsed 2m 47s (remain 3m 32s) Loss: 0.0787(0.1014) Grad: 165026.1562  LR: 0.00001427  \n",
      "Epoch: [2][180/366] Elapsed 3m 5s (remain 3m 10s) Loss: 0.0624(0.1012) Grad: 35575.5898  LR: 0.00001388  \n",
      "Epoch: [2][200/366] Elapsed 3m 26s (remain 2m 49s) Loss: 0.0673(0.1005) Grad: 38895.3750  LR: 0.00001348  \n",
      "Epoch: [2][220/366] Elapsed 3m 47s (remain 2m 29s) Loss: 0.0683(0.1009) Grad: 41513.1602  LR: 0.00001308  \n",
      "Epoch: [2][240/366] Elapsed 4m 7s (remain 2m 8s) Loss: 0.0674(0.1010) Grad: 31589.7129  LR: 0.00001267  \n",
      "Epoch: [2][260/366] Elapsed 4m 26s (remain 1m 47s) Loss: 0.0763(0.1011) Grad: 44496.3555  LR: 0.00001225  \n",
      "Epoch: [2][280/366] Elapsed 4m 48s (remain 1m 27s) Loss: 0.1029(0.1007) Grad: 81588.1953  LR: 0.00001183  \n",
      "Epoch: [2][300/366] Elapsed 5m 8s (remain 1m 6s) Loss: 0.0734(0.0999) Grad: 24712.1055  LR: 0.00001141  \n",
      "Epoch: [2][320/366] Elapsed 5m 28s (remain 0m 45s) Loss: 0.1049(0.0999) Grad: 37436.1992  LR: 0.00001098  \n",
      "Epoch: [2][340/366] Elapsed 5m 49s (remain 0m 25s) Loss: 0.1116(0.0994) Grad: 45035.1172  LR: 0.00001056  \n",
      "Epoch: [2][360/366] Elapsed 6m 9s (remain 0m 5s) Loss: 0.0872(0.0989) Grad: 49313.0273  LR: 0.00001013  \n",
      "Epoch: [2][365/366] Elapsed 6m 13s (remain 0m 0s) Loss: 0.0743(0.0987) Grad: 28143.2812  LR: 0.00001002  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 25s) Loss: 0.1309(0.1309) \n",
      "EVAL: [20/62] Elapsed 0m 28s (remain 0m 54s) Loss: 0.0888(0.1075) \n",
      "EVAL: [40/62] Elapsed 0m 55s (remain 0m 28s) Loss: 0.1276(0.1109) \n",
      "EVAL: [60/62] Elapsed 1m 22s (remain 0m 1s) Loss: 0.0854(0.1094) \n",
      "EVAL: [61/62] Elapsed 1m 22s (remain 0m 0s) Loss: 0.0356(0.1092) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0987  avg_val_loss: 0.1092  time: 457s\n",
      "Epoch 2 - Score: 0.4691  Scores: [0.5020485545096478, 0.4613233953088541, 0.4390017136544526, 0.4674648733048099, 0.48941377804783537, 0.45550117561165127]\n",
      "Epoch 2 - Save Best Score: 0.4691 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/366] Elapsed 0m 1s (remain 10m 47s) Loss: 0.1291(0.1291) Grad: inf  LR: 0.00001000  \n",
      "Epoch: [3][20/366] Elapsed 0m 21s (remain 5m 56s) Loss: 0.1099(0.0834) Grad: 102910.4141  LR: 0.00000957  \n",
      "Epoch: [3][40/366] Elapsed 0m 39s (remain 5m 13s) Loss: 0.0825(0.0780) Grad: 55956.2578  LR: 0.00000914  \n",
      "Epoch: [3][60/366] Elapsed 1m 1s (remain 5m 6s) Loss: 0.0773(0.0782) Grad: 84161.1484  LR: 0.00000872  \n",
      "Epoch: [3][80/366] Elapsed 1m 22s (remain 4m 51s) Loss: 0.0814(0.0801) Grad: 134037.2656  LR: 0.00000829  \n",
      "Epoch: [3][100/366] Elapsed 1m 45s (remain 4m 35s) Loss: 0.0746(0.0793) Grad: 97301.9219  LR: 0.00000787  \n",
      "Epoch: [3][120/366] Elapsed 2m 5s (remain 4m 13s) Loss: 0.0728(0.0795) Grad: 71563.6875  LR: 0.00000746  \n",
      "Epoch: [3][140/366] Elapsed 2m 24s (remain 3m 51s) Loss: 0.0567(0.0784) Grad: 68603.6016  LR: 0.00000704  \n",
      "Epoch: [3][160/366] Elapsed 2m 44s (remain 3m 29s) Loss: 0.0863(0.0780) Grad: 160311.4219  LR: 0.00000664  \n",
      "Epoch: [3][180/366] Elapsed 3m 2s (remain 3m 6s) Loss: 0.0437(0.0769) Grad: 82657.9062  LR: 0.00000624  \n",
      "Epoch: [3][200/366] Elapsed 3m 27s (remain 2m 49s) Loss: 0.0857(0.0778) Grad: 90199.0859  LR: 0.00000584  \n",
      "Epoch: [3][220/366] Elapsed 3m 49s (remain 2m 30s) Loss: 0.1164(0.0782) Grad: 156592.3125  LR: 0.00000546  \n",
      "Epoch: [3][240/366] Elapsed 4m 10s (remain 2m 9s) Loss: 0.0695(0.0785) Grad: 122179.2344  LR: 0.00000508  \n",
      "Epoch: [3][260/366] Elapsed 4m 28s (remain 1m 48s) Loss: 0.1111(0.0784) Grad: 136817.7969  LR: 0.00000471  \n",
      "Epoch: [3][280/366] Elapsed 4m 49s (remain 1m 27s) Loss: 0.0661(0.0781) Grad: 26747.1738  LR: 0.00000435  \n",
      "Epoch: [3][300/366] Elapsed 5m 8s (remain 1m 6s) Loss: 0.0626(0.0780) Grad: 64811.6680  LR: 0.00000400  \n",
      "Epoch: [3][320/366] Elapsed 5m 29s (remain 0m 46s) Loss: 0.0716(0.0778) Grad: 49867.4453  LR: 0.00000367  \n",
      "Epoch: [3][340/366] Elapsed 5m 49s (remain 0m 25s) Loss: 0.0791(0.0778) Grad: 57816.4648  LR: 0.00000334  \n",
      "Epoch: [3][360/366] Elapsed 6m 8s (remain 0m 5s) Loss: 0.0638(0.0774) Grad: 42989.7773  LR: 0.00000303  \n",
      "Epoch: [3][365/366] Elapsed 6m 14s (remain 0m 0s) Loss: 0.0887(0.0776) Grad: 72075.4219  LR: 0.00000295  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 26s) Loss: 0.1302(0.1302) \n",
      "EVAL: [20/62] Elapsed 0m 28s (remain 0m 54s) Loss: 0.0972(0.1076) \n",
      "EVAL: [40/62] Elapsed 0m 55s (remain 0m 28s) Loss: 0.1309(0.1093) \n",
      "EVAL: [60/62] Elapsed 1m 22s (remain 0m 1s) Loss: 0.0843(0.1079) \n",
      "EVAL: [61/62] Elapsed 1m 22s (remain 0m 0s) Loss: 0.0404(0.1078) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0776  avg_val_loss: 0.1078  time: 457s\n",
      "Epoch 3 - Score: 0.4658  Scores: [0.49818176717137325, 0.45720928505874914, 0.4354261865872814, 0.45778207723611886, 0.48439806409800545, 0.4618206706460046]\n",
      "Epoch 3 - Save Best Score: 0.4658 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/366] Elapsed 0m 1s (remain 9m 41s) Loss: 0.0548(0.0548) Grad: 116350.3203  LR: 0.00000294  \n",
      "Epoch: [4][20/366] Elapsed 0m 20s (remain 5m 34s) Loss: 0.0945(0.0653) Grad: 185711.0781  LR: 0.00000264  \n",
      "Epoch: [4][40/366] Elapsed 0m 41s (remain 5m 29s) Loss: 0.0746(0.0701) Grad: 72668.3203  LR: 0.00000236  \n",
      "Epoch: [4][60/366] Elapsed 1m 2s (remain 5m 13s) Loss: 0.0769(0.0710) Grad: 165328.3438  LR: 0.00000209  \n",
      "Epoch: [4][80/366] Elapsed 1m 23s (remain 4m 55s) Loss: 0.0554(0.0697) Grad: 94074.9688  LR: 0.00000183  \n",
      "Epoch: [4][100/366] Elapsed 1m 43s (remain 4m 32s) Loss: 0.0545(0.0688) Grad: 71667.5859  LR: 0.00000159  \n",
      "Epoch: [4][120/366] Elapsed 2m 2s (remain 4m 8s) Loss: 0.0760(0.0688) Grad: 72818.8281  LR: 0.00000137  \n",
      "Epoch: [4][140/366] Elapsed 2m 21s (remain 3m 46s) Loss: 0.0496(0.0689) Grad: 53871.7266  LR: 0.00000116  \n",
      "Epoch: [4][160/366] Elapsed 2m 42s (remain 3m 26s) Loss: 0.0701(0.0689) Grad: 120543.3125  LR: 0.00000097  \n",
      "Epoch: [4][180/366] Elapsed 3m 3s (remain 3m 7s) Loss: 0.0684(0.0697) Grad: 100629.6641  LR: 0.00000079  \n",
      "Epoch: [4][200/366] Elapsed 3m 25s (remain 2m 48s) Loss: 0.0605(0.0697) Grad: 96162.4844  LR: 0.00000063  \n",
      "Epoch: [4][220/366] Elapsed 3m 45s (remain 2m 28s) Loss: 0.0561(0.0694) Grad: 96958.2344  LR: 0.00000049  \n",
      "Epoch: [4][240/366] Elapsed 4m 4s (remain 2m 7s) Loss: 0.0606(0.0694) Grad: 82334.2188  LR: 0.00000037  \n",
      "Epoch: [4][260/366] Elapsed 4m 24s (remain 1m 46s) Loss: 0.0766(0.0696) Grad: 66729.6953  LR: 0.00000026  \n",
      "Epoch: [4][280/366] Elapsed 4m 44s (remain 1m 26s) Loss: 0.0729(0.0695) Grad: 54917.1172  LR: 0.00000017  \n",
      "Epoch: [4][300/366] Elapsed 5m 3s (remain 1m 5s) Loss: 0.0739(0.0691) Grad: 78886.5391  LR: 0.00000010  \n",
      "Epoch: [4][320/366] Elapsed 5m 23s (remain 0m 45s) Loss: 0.0651(0.0690) Grad: 147538.5469  LR: 0.00000005  \n",
      "Epoch: [4][340/366] Elapsed 5m 44s (remain 0m 25s) Loss: 0.0581(0.0689) Grad: 73891.4609  LR: 0.00000002  \n",
      "Epoch: [4][360/366] Elapsed 6m 7s (remain 0m 5s) Loss: 0.0481(0.0686) Grad: 65090.0586  LR: 0.00000000  \n",
      "Epoch: [4][365/366] Elapsed 6m 11s (remain 0m 0s) Loss: 0.0404(0.0684) Grad: 57153.4258  LR: 0.00000000  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 25s) Loss: 0.1108(0.1108) \n",
      "EVAL: [20/62] Elapsed 0m 28s (remain 0m 54s) Loss: 0.1092(0.1063) \n",
      "EVAL: [40/62] Elapsed 0m 55s (remain 0m 28s) Loss: 0.1433(0.1083) \n",
      "EVAL: [60/62] Elapsed 1m 22s (remain 0m 1s) Loss: 0.0828(0.1072) \n",
      "EVAL: [61/62] Elapsed 1m 22s (remain 0m 0s) Loss: 0.0440(0.1070) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0684  avg_val_loss: 0.1070  time: 455s\n",
      "Epoch 4 - Score: 0.4639  Scores: [0.494342279197483, 0.45934233850396916, 0.42832823860701014, 0.4597823294633263, 0.4857399590049584, 0.456092299660875]\n",
      "Epoch 4 - Save Best Score: 0.4639 Model\n",
      "========== fold: 3 result ==========\n",
      "Score: 0.4639  Scores: [0.494342279197483, 0.45934233850396916, 0.42832823860701014, 0.4597823294633263, 0.4857399590049584, 0.456092299660875]\n",
      "========== CV ==========\n",
      "Score: 0.4645  Scores: [0.49386353969364916, 0.4558229943426986, 0.423358743899167, 0.466020253549629, 0.4865422567772854, 0.46164768889322527]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finised\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    def get_result(oof_df):\n",
    "        labels = oof_df[CFG.target_cols].values\n",
    "        preds = oof_df[[f\"pred_{c}\" for c in CFG.target_cols]].values\n",
    "        score, scores = get_score(labels, preds)\n",
    "        LOGGER.info(f'Score: {score:<.4f}  Scores: {scores}')\n",
    "    \n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                _oof_df = train_loop(train, fold)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "                get_result(_oof_df)\n",
    "        oof_df = oof_df.reset_index(drop=True)\n",
    "        LOGGER.info(f\"========== CV ==========\")\n",
    "        get_result(oof_df)\n",
    "        oof_df.to_pickle(OUTPUT_DIR+'oof_df.pkl')\n",
    "        \n",
    "    print('finised')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251828a2-f86b-44e3-ae54-561b93f18715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be9cd73-62ae-4435-8a85-332d29a1de4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
