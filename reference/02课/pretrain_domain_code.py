# -*- coding: utf-8 -*-
"""pretrain.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zJJnT_Sg4A7i9ST1uv7FwooeY1qW8HLw
"""

import json
from torch.utils.data import DataLoader, Dataset
import torch
from transformers import AutoTokenizer, DataCollatorForLanguageModeling, DataCollatorWithPadding, Trainer, \
    AutoModelWithLMHead, TrainingArguments, AutoConfig,RobertaTokenizer,RobertaForMaskedLM
import pandas as pd
from pathlib import Path

pd.set_option('max_colwidth', 300)

# !wget https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/python.zip

# !unzip python.zip

import ast

def read_jsonfile(file_name):
    data = []
    with open(file_name, encoding='utf-8') as f:
        data = json.loads(f.read(), strict=False)
    return data

pydf = pd.read_csv('train_with_fold.csv')

pydf = pydf[['full_text']]
pydf = pydf.dropna()
pydf['source'] = pydf[['full_text']]

tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=True, mlm_probability=0.15
)


class PDataset(Dataset):

    def __init__(self, df, tokenizer):
        super().__init__()
        self.df = df.reset_index(drop=True)
        # maxlen allowed by model config
        self.tokenizer = tokenizer

    def __getitem__(self, index):
        row = self.df.iloc[index]
        doc = row.source
       
        sep = self.tokenizer.sep_token_id
        cls = self.tokenizer.cls_token_id
        inputs = {}
        
        doc_id = tokenizer(doc, truncation=True, max_length=512)
        doc_id = data_collator([doc_id])
        inputs['input_ids'] = doc_id['input_ids'][0].tolist()
        inputs['labels'] = doc_id['labels'][0].tolist()
    
        if 'token_type_ids' in inputs:
            inputs['token_type_ids'] = [0] * len(inputs['input_ids'])
    
        
        return inputs

    def __len__(self):
        return self.df.shape[0]


mask_id = tokenizer.mask_token_id
def data_collator_p(batch):
    max_length = max([len(i['input_ids']) for i in batch])
    input_id, token_type, labels = [], [], []
    for i in batch:
        input_id.append(i['input_ids'] + [mask_id]*(max_length-len(i['input_ids'])))
        #token_type.append(i['token_type_ids'] + [1] * (max_length - len(i['token_type_ids'])))
        labels.append(i['labels'] + [-100] * (max_length - len(i['labels'])))
    output={}
    output['input_ids'] = torch.as_tensor(input_id, dtype=torch.long)
    #output['token_type_ids'] = torch.as_tensor(token_type, dtype=torch.long)
    output['labels'] = torch.as_tensor(labels, dtype=torch.long)
    return output

training_args = TrainingArguments(
    output_dir='./pretrain_domain_code',
    overwrite_output_dir=True,
    num_train_epochs=4,
    per_device_train_batch_size=16,
    save_total_limit=4,
    save_strategy='epoch',
    learning_rate=2e-5,
    # fp16=True,
    gradient_accumulation_steps=4,
)
dataset = PDataset(pydf, tokenizer)

model = AutoModelWithLMHead.from_pretrained('microsoft/deberta-v3-base')
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator_p,
    train_dataset=dataset,
)
trainer.train()
trainer.save_model('./pretrain_domain_code')
